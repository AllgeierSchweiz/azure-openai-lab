{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfc07dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python packages\n",
    "import praw\n",
    "from datetime import datetime\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "from io import StringIO\n",
    "import json\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from openai import AzureOpenAI\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4b52b2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load required variables from .env file.\n",
    "load_dotenv(dotenv_path=Path(\"C:\\\\Python\\\\azure-openai-lab\\\\.venv\\\\.env\")) #Error sometimes due to \\ or \\\\. Try one or the other. \"C:\\\\Python\\\\azure-openai-lab\\\\.venv\\\\.env\"\n",
    "\n",
    "# Load Azure OpenAI Key and Endpoint. These values can be found within the Azure OpenAI Service resource in portal.azure.com under Keys and Endpoint\n",
    "azure_oai_key = os.environ['AZURE_OPENAI_KEY_P34']\n",
    "azure_oai_endpoint = os.environ['AZURE_OPENAI_ENDPOINT_P34']\n",
    "\n",
    "reddit_api_id = os.environ['REDDIT_API_ID']\n",
    "reddit_api_secret = os.environ['REDDIT_API_SECRET']\n",
    "reddit_api_user = os.environ['REDDIT_API_USER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a0fd819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Azure OpenAI client\n",
    "client = AzureOpenAI(\n",
    "        azure_endpoint = azure_oai_endpoint, \n",
    "        api_key=azure_oai_key,  \n",
    "        api_version=\"2024-10-21\" #\"2024-02-01\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee940ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1) Authenticate\n",
    "reddit = praw.Reddit(\n",
    "    client_id = reddit_api_id,\n",
    "    client_secret = reddit_api_secret,\n",
    "    user_agent = \"CommentRetriever/0.1 by u/\" + reddit_api_user\n",
    ")\n",
    "\n",
    "# 2) Fetch the submission\n",
    "submission = reddit.submission(url=\"https://www.reddit.com/r/PowerBI/comments/1k2upuw/does_upgrading_to_fabric_capacity_workspace_make/\")\n",
    "\n",
    "# 4) Start building the output string with the post metadata and body\n",
    "lines = []\n",
    "post_time = datetime.utcfromtimestamp(submission.created_utc).isoformat()\n",
    "lines.append(f\"Post: {submission.title} (u/{submission.author})\")\n",
    "lines.append(f\"Time: {post_time}\")\n",
    "lines.append(f\"URL:  {submission.url}\\n\")\n",
    "lines.append(submission.selftext or \"[no body text]\")\n",
    "lines.append(\"\\n---\\nComments:\")\n",
    "\n",
    "# 5) Load all comments\n",
    "submission.comments.replace_more(limit=None)\n",
    "\n",
    "# 6) Flatten and append each comment to output\n",
    "for comment in submission.comments.list():\n",
    "    c_time = datetime.utcfromtimestamp(comment.created_utc).isoformat()\n",
    "    author = comment.author or \"[deleted]\"\n",
    "    body   = comment.body.replace('\\n', ' ')\n",
    "    lines.append(f\"[{c_time}] u/{author}: {body}\")\n",
    "\n",
    "output = \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00932d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create advanced System prompt\n",
    "systemcontent = \\\n",
    "\"\"\"\n",
    "### INSTRUCTIONS\n",
    "Persona: Act as social media expert Dr. Rachna Jain who specializes in strategies for using psychological principles to enhance social media content.\n",
    "Action: Create a social media post telling a story in the active voice that provide practical business value by Focusing on a single and clear Goal. The structure should be authentic and in the third person. The Text should be clear and professional (no emojis). Use comments that answer the question posed by the post and summarize these as solutions.\n",
    "Target Audience: The recipients of the post are business leaders, managers and professionals who are curious about the topic.\n",
    "\n",
    "### EXAMPLE\n",
    "\n",
    "Title\n",
    "\n",
    "Story-telling...\n",
    "\n",
    "Implications...\n",
    "\n",
    "1. Negative effect 1\n",
    "\n",
    "2. Negative effect 2\n",
    "\n",
    "3. Negative effect 3\n",
    "\n",
    "Counteracting...\n",
    "\n",
    "1. Solution 1\n",
    "\n",
    "2. Solution 2\n",
    "\n",
    "3. Solution 3\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9bca1898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing Data Storage with PowerBI: Is Fabric Capacity Right for Your Business?\n",
      "\n",
      "Did you know that standard PowerBI Pro licenses have dataset size limitations that can restrict your ability to store extensive historical data?\n",
      "\n",
      "Implications of staying with PowerBI Pro workspace:\n",
      "\n",
      "1. Dataset size limitations may prevent long-term storage of sensor data from multiple plants.\n",
      "2. Frequent deletion and reloading of tables consumes significant resources and reduces efficiency.\n",
      "3. Potential loss of valuable historical insights due to limited data retention.\n",
      "\n",
      "Counteracting these limitations effectively:\n",
      "\n",
      "1. Consider upgrading to Fabric capacity workspace, allowing larger dataset storage and incremental refresh capabilities.\n",
      "2. Evaluate your specific data requirements and choose a Fabric SKU that aligns with your storage and performance needs.\n",
      "3. Implement incremental refresh strategies available in both Pro and Fabric capacities to optimize data management and reduce resource usage.\n",
      "\n",
      "Community insights summarized from discussions:\n",
      "\n",
      "- Fabric capacity allows for larger semantic models, enabling storage of extensive historical data and incremental refreshes (u/Azured_).\n",
      "- Incremental refresh is supported in Pro workspaces, but dataset size limits may still pose constraints for large sensor datasets (u/SQLGene).\n",
      "- Selecting a smaller Fabric capacity SKU specifically for persistent data sources can help manage costs effectively (u/Azured_).\n",
      "\n",
      "Has your organization faced similar challenges with dataset limitations? What solutions have you implemented to overcome these constraints? Share your experiences below.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Zero-Shot learning. Model has a token limit of 4096.\n",
    "\n",
    "# Send request to Azure OpenAI model\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4.5-preview\",       # or whichever model you prefer\n",
    "    temperature=0.4,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": systemcontent\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"Create a post:\\n\"\n",
    "                + \"---\\n\"\n",
    "                + output\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "result = response.choices[0].message.content\n",
    "print(result + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
