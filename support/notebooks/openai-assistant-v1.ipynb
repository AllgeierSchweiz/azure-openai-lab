{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b812b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#————————————————————\n",
    "\n",
    "# Name: OpenAI Assistant (V1)\n",
    "\n",
    "# Purpose:\n",
    "\n",
    "# This notebook will create a GPT Assistant data engineer using the OpenAI API, give it a data set, ask it to process it using it's instructions, and then prepare the resulting dataframe for download. The idea here is to test if the assistant can successfully complete data prep and some data engineering, then reduce the feature set using a logistic regression with LASSO regularization, subsetting only non-zero LASSO coefficient features in the final dataset. \n",
    "\n",
    "# This notebook will create a GPT Assistant using OpenAI's API and provide it with the training dataframe returned by the data engineer Assistant and a set of instructions to creating an \"Extra Trees\" Random Forest. Basic outline of instructions for the modeler:\n",
    "\n",
    "# 1. Load the provided dataframe into a pandas df.\n",
    "# 2. Split the data set into training and testing using a 75:25 split.\n",
    "# 3. Train an Extra Trees random forest with 2000 trees.\n",
    "# 4. Use the testing data to measure the model's accuracy, presicion, recall, and generate a confusion matrix.\n",
    "# 5. Return the results in a single csv table. \n",
    "\n",
    "# Company: Allgeier Schweiz AG\n",
    "# Author: Nicolas Rehder (nrehder@allgeier.ch)\n",
    "# Create for: SDSC 2024\n",
    "# Date Created: 22.01.2024\n",
    "# Last Updated: 22.01.2024\n",
    "# Python Version: 3.10.4\n",
    "\n",
    "#General Sources:\n",
    "#https://platform.openai.com/docs/api-reference?lang=python\n",
    "#https://medium.com/ai-advances/complete-process-for-fine-tuning-gpt-3-5-turbo-using-openai-api-db4a50b3de1a\n",
    "\n",
    "#Openai Usage:\n",
    "#https://platform.openai.com/usage\n",
    "\n",
    "#Additionals:\n",
    "# - https://platform.openai.com/docs/models\n",
    "# - https://openai.com/pricing\n",
    "\n",
    "# Download Python packages (run the below command in terminal if packages have not yet been installed)\n",
    "#pip install -r C:\\Python\\openai-lab\\support\\requirements\\requirements.txt\n",
    "\n",
    "#————————————————————"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aaf4eb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "from io import StringIO\n",
    "import json\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import openai\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "830eab53-b204-4d22-a715-372f2799f0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OpenAIKey\n",
    "load_dotenv(dotenv_path=Path(\"C:\\Python\\openai-lab\\.venv\\.env\"))\n",
    "API_KEY = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# Initalise Large Language Model (LLM)\n",
    "\n",
    "client = OpenAI(api_key = API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "78367c55-8ca2-4e2b-b651-05fce7b5c9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Functions\n",
    "\n",
    "def read_and_save_file(first_file_id, file_name):    \n",
    "    # its binary, so read it and then make it a file like object\n",
    "    file_data = client.files.content(first_file_id)\n",
    "    file_data_bytes = file_data.read()\n",
    "    file_like_object = io.BytesIO(file_data_bytes)\n",
    "    #now read as csv to create df\n",
    "    returned_data = pd.read_csv(file_like_object)\n",
    "    returned_data.to_csv(file_name, index=False)\n",
    "    return returned_data\n",
    "    # file = read_and_save_file(first_file_id, \"analyst_output.csv\")\n",
    "    \n",
    "def files_from_messages(messages, asst_name):\n",
    "    first_thread_message = messages.data[0]  # Accessing the first ThreadMessage\n",
    "    message_ids = first_thread_message.file_ids\n",
    "    print(message_ids)\n",
    "    # Loop through each file ID and save the file with a sequential name\n",
    "    for i, file_id in enumerate(message_ids):\n",
    "        file_name = f\"{asst_name}_output_{i+1}.csv\"  # Generate a sequential file name\n",
    "        read_and_save_file(file_id, file_name)\n",
    "        print(f'saved {file_name}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "05ab7949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "path_input = r\"C:\\Python\\data\\openfoodfacts.csv\" #Change path if required\n",
    "#df = pd.read_csv(path_input , sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41bde8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileObject(id='file-mdjvXQoyO0Upxe7LmLRTBmNs', bytes=291784575, created_at=1706088798, filename='openfoodfacts.csv', object='file', purpose='assistants', status='processed', status_details=None)\n",
      "Assistant(id='asst_uggm5vBAiO5NRdfwPyW8cbtv', created_at=1706088804, description=None, file_ids=['file-mdjvXQoyO0Upxe7LmLRTBmNs'], instructions='\\nYou are a senior data engineer who will work with data in a csv file in your files. \\nWhen the user asks you to perform your actions, use the csv file to read the data into a pandas dataframe.\\nYou will perform data cleansing and transformation steps. The data set is to be used for a classification model.\\nExecute each of the steps listed below in your ACTIONS section. The user will identify the target variable. \\n\\nACTIONS:\\n\\n1. Read the file data into a pandas DataFrame by tab separated values.\\n2. Keep only the columns \"product_name\" and \"categories_tags\".\\n3. Filter the column \"categories_tags\" and only show values that contain 3 commas.\\n4. Remove empty or NA rows from columns \"product_name\" and \"categories_tags\".\\n5. Remove rows with non-english words from columns \"product_name\" and \"categories_tags\".\\n6. Replace the characters \"en:\" in column \"categories_tags\" with blank.\\n7. Remove stop words and adjectives from column \"product_name\".\\n8. Remove duplicate values from column \"product_name\".\\n9. Deduplicate the column \"product_name\" using an optimized method of fuzzy string matching. Similarly named rows should only appear once.\\n10. Trim and lowercase the values of all columns and prepare the results as Table_1.\\n11. Prepare Table_1 as a csv file for download by the user. \\n12. Provide a summary paragraph explaining the preparation of the data set.\\n\\nDO NOT:\\n1. Do not return any images. \\n2. Do not return any other file types.\\n', metadata={}, model='gpt-4-1106-preview', name='data_assistant', object='assistant', tools=[ToolCodeInterpreter(type='code_interpreter')])\n"
     ]
    }
   ],
   "source": [
    "# create the assistant and give it the CSV file\n",
    "\n",
    "mls = '''\n",
    "You are a senior data engineer who will work with data in a csv file in your files. \n",
    "When the user asks you to perform your actions, use the csv file to read the data into a pandas dataframe.\n",
    "You will perform data cleansing and transformation steps. The data set is to be used for a classification model.\n",
    "Execute each of the steps listed below in your ACTIONS section. The user will identify the target variable. \n",
    "\n",
    "ACTIONS:\n",
    "\n",
    "1. Read the file data into a pandas DataFrame by tab separated values.\n",
    "2. Keep only the columns \"product_name\" and \"categories_tags\".\n",
    "3. Filter the column \"categories_tags\" and only show values that contain 3 commas.\n",
    "4. Remove empty or NA rows from columns \"product_name\" and \"categories_tags\".\n",
    "5. Remove rows with non-english words from columns \"product_name\" and \"categories_tags\".\n",
    "6. Replace the characters \"en:\" in column \"categories_tags\" with blank.\n",
    "7. Remove stop words and adjectives from column \"product_name\".\n",
    "8. Remove duplicate values from column \"product_name\".\n",
    "9. Deduplicate the column \"product_name\" using an optimized method of fuzzy string matching. Similarly named rows should only appear once.\n",
    "10. Trim and lowercase the values of all columns and prepare the results as Table_1.\n",
    "11. Prepare Table_1 as a csv file for download by the user. \n",
    "12. Provide a summary paragraph explaining the preparation of the data set.\n",
    "\n",
    "DO NOT:\n",
    "1. Do not return any images. \n",
    "2. Do not return any other file types.\n",
    "'''\n",
    "\n",
    "# send the csv file to the assistant purpose files\n",
    "response = client.files.create(\n",
    "  file=open(path_input, \"rb\"),\n",
    "  purpose=\"assistants\"\n",
    ")\n",
    "print(response)\n",
    "file__id = response.id\n",
    "\n",
    "openai_assistant = client.beta.assistants.create(\n",
    "    instructions=mls,\n",
    "    name=\"data_assistant\",\n",
    "    tools=[{\"type\": \"code_interpreter\"}],\n",
    "    model=\"gpt-4-1106-preview\", # gpt-4\n",
    "    file_ids=[file__id]\n",
    ")\n",
    "\n",
    "# get the file id\n",
    "fileId = openai_assistant.file_ids[0]\n",
    "print(openai_assistant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1db6bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please execute your ACTIONS on the data stored in the csv file file-mdjvXQoyO0Upxe7LmLRTBmNs . The Target variable is categories_tags\n",
      "{\n",
      "    \"id\": \"run_ApuIO20zElhRDsfOUYSzNHMN\",\n",
      "    \"assistant_id\": \"asst_uggm5vBAiO5NRdfwPyW8cbtv\",\n",
      "    \"cancelled_at\": null,\n",
      "    \"completed_at\": null,\n",
      "    \"created_at\": 1706088868,\n",
      "    \"expires_at\": 1706089468,\n",
      "    \"failed_at\": null,\n",
      "    \"file_ids\": [\n",
      "        \"file-mdjvXQoyO0Upxe7LmLRTBmNs\"\n",
      "    ],\n",
      "    \"instructions\": \"\\nYou are a senior data engineer who will work with data in a csv file in your files. \\nWhen the user asks you to perform your actions, use the csv file to read the data into a pandas dataframe.\\nYou will perform data cleansing and transformation steps. The data set is to be used for a classification model.\\nExecute each of the steps listed below in your ACTIONS section. The user will identify the target variable. \\n\\nACTIONS:\\n\\n1. Read the file data into a pandas DataFrame by tab separated values.\\n2. Keep only the columns \\\"product_name\\\" and \\\"categories_tags\\\".\\n3. Filter the column \\\"categories_tags\\\" and only show values that contain 3 commas.\\n4. Remove empty or NA rows from columns \\\"product_name\\\" and \\\"categories_tags\\\".\\n5. Remove rows with non-english words from columns \\\"product_name\\\" and \\\"categories_tags\\\".\\n6. Replace the characters \\\"en:\\\" in column \\\"categories_tags\\\" with blank.\\n7. Remove stop words and adjectives from column \\\"product_name\\\".\\n8. Remove duplicate values from column \\\"product_name\\\".\\n9. Deduplicate the column \\\"product_name\\\" using an optimized method of fuzzy string matching. Similarly named rows should only appear once.\\n10. Trim and lowercase the values of all columns and prepare the results as Table_1.\\n11. Prepare Table_1 as a csv file for download by the user. \\n12. Provide a summary paragraph explaining the preparation of the data set.\\n\\nDO NOT:\\n1. Do not return any images. \\n2. Do not return any other file types.\\n\",\n",
      "    \"last_error\": null,\n",
      "    \"metadata\": {},\n",
      "    \"model\": \"gpt-4-1106-preview\",\n",
      "    \"object\": \"thread.run\",\n",
      "    \"required_action\": null,\n",
      "    \"started_at\": null,\n",
      "    \"status\": \"queued\",\n",
      "    \"thread_id\": \"thread_bpYGv9A3gpImt9KNRvsSJK2S\",\n",
      "    \"tools\": [\n",
      "        {\n",
      "            \"type\": \"code_interpreter\"\n",
      "        }\n",
      "    ],\n",
      "    \"usage\": null\n",
      "}\n",
      "60 seconds later...\n",
      "60 seconds later...\n",
      "60 seconds later...\n",
      "60 seconds later...\n",
      "60 seconds later...\n",
      "Assistant: The dataset has been successfully prepared with various data cleansing steps:\n",
      "\n",
      "1. Tab-delimited values were read into a pandas DataFrame.\n",
      "2. Retained only the columns \"product_name\" and \"categories_tags\".\n",
      "3. Filtered \"categories_tags\" to only include entries with exactly three commas.\n",
      "4. Removed rows with empty or NA values in \"product_name\" and \"categories_tags\".\n",
      "5. Took out rows with non-English characters, though due to the restriction, this was a basic implementation and may not be fully accurate.\n",
      "6. Replaced \"en:\" in \"categories_tags\" with an empty string.\n",
      "7. Removed stop words and special characters from \"product_name\" using a basic approach.\n",
      "8. Removed duplicate values in \"product_name\".\n",
      "9. The deduplication was done using a basic method due to lack of resources to perform fuzzy matching.\n",
      "10. Lowercased and trimmed strings in \"product_name\" and \"categories_tags\".\n",
      "11. Saved the cleaned data as 'cleaned_data.csv' for download.\n",
      "\n",
      "The dataset is now preprocessed and could be used for a classification model with \"categories_tags\" as the target variable, but it's important to note the compromises made and consider if further cleaning would be required for actual model training.\n",
      "\n",
      "You can download the cleaned CSV file from the following link: [cleaned_data.csv](sandbox:/mnt/data/cleaned_data.csv).\n",
      "Assistant: It seems that there was a problem with downloading NLTK resources required for stopwords and English words due to a lack of Internet access in the environment. Let's adjust our approach by using a more basic method that does not rely on those external resources for detecting English words and removing stop words. This will be a compromise, as the accuracy of English word detection and stop word removal might be slightly less than with the comprehensive lists provided by NLTK.\n",
      "\n",
      "We can continue by first removing the \"en:\" part from the \"categories_tags\" column, as this can definitely be identified without external resources. Based on this, we will then perform basic data cleansing operations to lowercase strings and remove duplicates.\n",
      "\n",
      "Let's proceed with the process and adjust accordingly.\n",
      "User: Please execute your ACTIONS on the data stored in the csv file file-mdjvXQoyO0Upxe7LmLRTBmNs . The Target variable is categories_tags\n"
     ]
    }
   ],
   "source": [
    "# make the request to the assistant\n",
    "\n",
    "message_string = \"Please execute your ACTIONS on the data stored in the csv file \" + fileId + \" . The Target variable is categories_tags\"\n",
    "print(message_string)\n",
    "\n",
    "# Step 2: Create a Thread\n",
    "thread = client.beta.threads.create()\n",
    "\n",
    "# Step 3: Add a Message to a Thread\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content= message_string\n",
    ")\n",
    "\n",
    "# Step 4: Run the Assistant\n",
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=openai_assistant.id\n",
    "    #instructions=\"Overwrite hard-coded instructions here\"\n",
    ")\n",
    "\n",
    "print(run.model_dump_json(indent=4))\n",
    "\n",
    "while True:\n",
    "    sec = 60\n",
    "    # Wait for 5 seconds\n",
    "    time.sleep(sec)  \n",
    "    # Retrieve the run status\n",
    "    run_status = client.beta.threads.runs.retrieve(\n",
    "        thread_id=thread.id,\n",
    "        run_id=run.id\n",
    "    )\n",
    "    print(f'{sec} seconds later...')\n",
    "    # If run is completed, get messages\n",
    "    if run_status.status == 'completed':\n",
    "        messages = client.beta.threads.messages.list(\n",
    "            thread_id=thread.id\n",
    "        )\n",
    "        # Loop through messages and print content based on role\n",
    "        for msg in messages.data:\n",
    "            role = msg.role\n",
    "            try:\n",
    "                content = msg.content[0].text.value\n",
    "                print(f\"{role.capitalize()}: {content}\")\n",
    "            except AttributeError:\n",
    "                # This will execute if .text does not exist\n",
    "                print(f\"{role.capitalize()}: [Non-text content, possibly an image or other file type]\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fec7ae93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['file-WHTNGh9hAYjDmdeNnqwcmZ6B']\n",
      "saved data_assistant_output_1.csv\n"
     ]
    }
   ],
   "source": [
    "# extract the file names from the response and retrieve the content\n",
    "asst_name = 'data_assistant'        \n",
    "files_from_messages(messages, asst_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c1e49dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>Table_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apple turnover</td>\n",
       "      <td>snacks,sweet-snacks,viennoiseries,apple-turnovers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bread</td>\n",
       "      <td>plant-based-foods-and-beverages,plant-based-fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>farmer cheese</td>\n",
       "      <td>dairies,fermented-foods,fermented-milk-product...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>german fine bread</td>\n",
       "      <td>plant-based-foods-and-beverages,plant-based-fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>today's temptations, lithuanian rye bread</td>\n",
       "      <td>plant-based-foods-and-beverages,plant-based-fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11227</th>\n",
       "      <td>special k, crunch hot cereal, maple brown suga...</td>\n",
       "      <td>plant-based-foods-and-beverages,plant-based-fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11228</th>\n",
       "      <td>kellogg's, knourish, hot cereal, cinnamon, rai...</td>\n",
       "      <td>plant-based-foods-and-beverages,plant-based-fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11229</th>\n",
       "      <td>hot cereal, cranberry, almond</td>\n",
       "      <td>plant-based-foods-and-beverages,plant-based-fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11230</th>\n",
       "      <td>kellogg's special k granola bars greek yogurt ...</td>\n",
       "      <td>snacks,sweet-snacks,bars,cereal-bars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11231</th>\n",
       "      <td>kellogg special k granola bars almond honey oa...</td>\n",
       "      <td>snacks,sweet-snacks,bars,cereal-bars</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11232 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            product_name  \\\n",
       "0                                         apple turnover   \n",
       "1                                                  bread   \n",
       "2                                          farmer cheese   \n",
       "3                                      german fine bread   \n",
       "4              today's temptations, lithuanian rye bread   \n",
       "...                                                  ...   \n",
       "11227  special k, crunch hot cereal, maple brown suga...   \n",
       "11228  kellogg's, knourish, hot cereal, cinnamon, rai...   \n",
       "11229                      hot cereal, cranberry, almond   \n",
       "11230  kellogg's special k granola bars greek yogurt ...   \n",
       "11231  kellogg special k granola bars almond honey oa...   \n",
       "\n",
       "                                                 Table_1  \n",
       "0      snacks,sweet-snacks,viennoiseries,apple-turnovers  \n",
       "1      plant-based-foods-and-beverages,plant-based-fo...  \n",
       "2      dairies,fermented-foods,fermented-milk-product...  \n",
       "3      plant-based-foods-and-beverages,plant-based-fo...  \n",
       "4      plant-based-foods-and-beverages,plant-based-fo...  \n",
       "...                                                  ...  \n",
       "11227  plant-based-foods-and-beverages,plant-based-fo...  \n",
       "11228  plant-based-foods-and-beverages,plant-based-fo...  \n",
       "11229  plant-based-foods-and-beverages,plant-based-fo...  \n",
       "11230               snacks,sweet-snacks,bars,cereal-bars  \n",
       "11231               snacks,sweet-snacks,bars,cereal-bars  \n",
       "\n",
       "[11232 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1 = pd.read_csv('data_assistant_output_1.csv')\n",
    "display(df1)\n",
    "\n",
    "# df2 = pd.read_csv('engineer_output_2.csv')\n",
    "# display(df2)\n",
    "\n",
    "# df3 = pd.read_csv('engineer_output_3.csv')\n",
    "# display(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315fb950",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.beta.assistants.delete(openai_assistant.id)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cdb7f7-a8f4-4ad9-bdba-900ab34c42d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load and check the file for the engineer\n",
    "asst_file = 'engineer_output_1.csv'\n",
    "df = pd.read_csv(asst_file)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1336118-fc38-4f8d-9115-1c294ad04a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the assistant and give it the CSV file\n",
    "\n",
    "mls = '''\n",
    "You are a data scientist who will build a predictive model with data from two csv files uploaded to your files. \n",
    "When the user asks you to perform your actions, use the csv file to read the data into a pandas dataframe.\n",
    "Then continue with each of the steps listed below in you ACTIONS. The user will identify the target variable. \n",
    "\n",
    "ACTIONS:\n",
    "\n",
    "1. Load the engineer_output_1 csv file into a pandas df of the same name.\n",
    "2. Split the data set into training and testing data sets with a 25% split.\n",
    "3. Train an Extra Trees random forest with 2000 trees\n",
    "4. Use the testing data to measure the models accuracy, presicion, recall, and confusion matrix.\n",
    "5. Format the testing data results as a csv table and prepare it for download by the user. \n",
    "\n",
    "DO NOT:\n",
    "1. Return any images. \n",
    "'''\n",
    "\n",
    "# send the csv file to the assistant purpose files\n",
    "response = client.files.create(\n",
    "  file=open(asst_file, \"rb\"),\n",
    "  purpose=\"assistants\"\n",
    ")\n",
    "print(response)\n",
    "file_1_id = response.id\n",
    "\n",
    "my_assistant = client.beta.assistants.create(\n",
    "    instructions=mls,\n",
    "    name=\"modeler_1\",\n",
    "    tools=[{\"type\": \"code_interpreter\"}],\n",
    "    model=\"gpt-4-1106-preview\", # gpt-4\n",
    "    file_ids=[file_1_id] # multiple files: file_ids=[file_1_id, file_2_id]\n",
    ")\n",
    "\n",
    "# get the file id\n",
    "fileId = my_assistant.file_ids[0]\n",
    "print(my_assistant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5cdf49-54c9-417e-a18d-43fd45c2b9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the request to the assistant\n",
    "\n",
    "message_string = \"Please execute your ACTIONS on \" + fileId + \" and prepare the resulting table for csv download. The Target variable is Class\"\n",
    "print(message_string)\n",
    "\n",
    "# Step 2: Create a Thread\n",
    "thread = client.beta.threads.create()\n",
    "\n",
    "# Step 3: Add a Message to a Thread\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content= message_string\n",
    ")\n",
    "\n",
    "# Step 4: Run the Assistant\n",
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=my_assistant.id\n",
    "    #instructions=\"Overwrite hard-coded instructions here\"\n",
    ")\n",
    "\n",
    "print(run.model_dump_json(indent=4))\n",
    "\n",
    "while True:\n",
    "    # Wait in between tries\n",
    "    sec = 60\n",
    "    time.sleep(sec)  \n",
    "    # Retrieve the run status\n",
    "    run_status = client.beta.threads.runs.retrieve(\n",
    "        thread_id=thread.id,\n",
    "        run_id=run.id\n",
    "    )\n",
    "    print('One eternity later...')\n",
    "    # If run is completed, get messages\n",
    "    if run_status.status == 'completed':\n",
    "        messages = client.beta.threads.messages.list(\n",
    "            thread_id=thread.id\n",
    "        )\n",
    "        # Loop through messages and print content based on role\n",
    "        for msg in messages.data:\n",
    "            role = msg.role\n",
    "            try:\n",
    "                content = msg.content[0].text.value\n",
    "                print(f\"{role.capitalize()}: {content}\")\n",
    "            except AttributeError:\n",
    "                # This will execute if .text does not exist\n",
    "                print(f\"{role.capitalize()}: [Non-text content, possibly an image or other file type]\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b542310-582c-4f97-a69b-1b2323a784af",
   "metadata": {},
   "outputs": [],
   "source": [
    "asst_name = 'modeler'        \n",
    "files_from_messages(messages, asst_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be18cde6-00c1-41d6-b346-d068438b05df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('modeler_output_1.csv')\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c113f438-97c8-4482-b00b-5ebce8b3882d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean up the assistant\n",
    "\n",
    "response = client.beta.assistants.delete(my_assistant.id)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
