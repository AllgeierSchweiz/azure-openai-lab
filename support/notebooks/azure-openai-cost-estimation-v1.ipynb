{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "#————————————————————\n",
    "\n",
    "# Name: Azure OpenAI Cost Estimation (V1)\n",
    "\n",
    "# Purpose: \n",
    "\n",
    "# Verify token count and estimate cost.\n",
    "\n",
    "# Company: Allgeier Schweiz AG\n",
    "# Author: Nicolas Rehder (nrehder@allgeier.ch)\n",
    "# Create for: SDSC 2024\n",
    "# Date Created: 10.01.2024\n",
    "# Last Updated: 10.01.2024\n",
    "# Python Version: 3.10.4\n",
    "\n",
    "# General Sources:\n",
    "# https://sdk.vercel.ai/\n",
    "# https://github.com/LazaUK/AOAI-Streaming-TokenUsage/tree/main\n",
    "\n",
    "# Azure Openai Usage:\n",
    "# https://community.openai.com/t/whats-the-gpt-4-turbo-encoding/505059\n",
    "# https://demiliani.com/2023/12/19/monitoring-your-azure-openai-usage/\n",
    "# https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/\n",
    "# https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models\n",
    "\n",
    "# Additionals:\n",
    "\n",
    "# Download Python packages (run the below command in terminal if packages have not yet been installed)\n",
    "# pip install -r C:\\Python\\openai-lab\\support\\requirements\\requirements.txt\n",
    "\n",
    "#————————————————————"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "from io import StringIO\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from openai import AzureOpenAI\n",
    "import tiktoken\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = '''\n",
    "You are a senior data analyst who will work with data in an xlsx file.\n",
    "You have access to a sandboxed environment for writing python code.\n",
    "When the user asks you to perform your actions, you will use the provided xlsx file.\n",
    "You will perform data cleansing and transformation steps.\n",
    "Execute each of the steps listed below in your ACTIONS section.\n",
    "\n",
    "ACTIONS:\n",
    "\n",
    "1. Read the xlsx file into a pandas DataFrame.\n",
    "2. Keep only the columns \"product_name\", \"level_1\", \"level_2\", \"level_3\".\n",
    "3. Trim and lowercase the values of columns \"product_name\", \"level_1\", \"level_2\", \"level_3\".\n",
    "4. Remove non-alphanumeric characters from column \"product_name\".\n",
    "5. Remove empty or NA rows from columns \"product_name\", \"level_1\", \"level_2\", \"level_3\".\n",
    "6. Remove duplicate values from column \"product_name\" and prepare the results as Table_1.\n",
    "7. Prepare Table_1 as an xlsx file for download by the user. \n",
    "8. Provide a summary paragraph explaining the preparation of the data set.\n",
    "\n",
    "DO NOT:\n",
    "1. Do not return any images. \n",
    "2. Do not return any other file types.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = [{\n",
    "   \"role\": \"user\",\n",
    "   \"content\": \"Explain to me how tolenization is working in OpenAi models?\",\n",
    "   }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\"):\n",
    "    \"\"\"\n",
    "    Return the number of tokens used by a list of messages.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    if model in {\n",
    "        \"gpt-3.5-turbo-0613\",\n",
    "        \"gpt-3.5-turbo-16k-0613\",\n",
    "        \"gpt-4-0314\",\n",
    "        \"gpt-4-32k-0314\",\n",
    "        \"gpt-4-0613\",\n",
    "        \"gpt-4-32k-0613\",\n",
    "        }:\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif \"gpt-3.5-turbo\" in model:\n",
    "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\")\n",
    "    elif \"gpt-4\" in model:\n",
    "        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\"\n",
    "        )\n",
    "    \n",
    "    num_tokens = 0\n",
    "\n",
    "    if type(messages) == list:\n",
    "        for message in messages:\n",
    "            num_tokens += tokens_per_message\n",
    "            for key, value in message.items():\n",
    "                num_tokens += len(encoding.encode(value))\n",
    "                if key == \"name\":\n",
    "                    num_tokens += tokens_per_name\n",
    "        num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    elif type(messages) == str:\n",
    "        num_tokens += len(encoding.encode(messages))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_from_messages(instructions, model=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate request cost (model example: gpt-4-0613)\n",
    "def cost(prompt_tokens):\n",
    "    # Price per 1000 tokens \n",
    "    input_price_per_1000 = 0.03\n",
    "    output_price_per_1000 = 0.06\n",
    "    code_interpreter = 0.03\n",
    "\n",
    "    # Output\n",
    "    completion_tokens = math.ceil(prompt_tokens * 0.2)\n",
    "    \n",
    "    # Calculate the cost for input and output tokens separately\n",
    "    input_cost = (prompt_tokens / 1000) * input_price_per_1000\n",
    "    output_cost = (completion_tokens / 1000) * output_price_per_1000\n",
    "    \n",
    "    # The total cost is the sum of input cost and output cost\n",
    "    total_cost = input_cost + output_cost + code_interpreter\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.5885"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost(num_tokens_from_messages(instructions, model=\"gpt-4\")) * 50"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
