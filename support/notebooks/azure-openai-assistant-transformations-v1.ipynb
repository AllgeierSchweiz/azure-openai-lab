{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#————————————————————\n",
    "\n",
    "# Name: Azure OpenAI Assistant for Data Transformations (V1)\n",
    "\n",
    "# Purpose:\n",
    "\n",
    "\n",
    "# Company: Allgeier Schweiz AG\n",
    "# Author: Nicolas Rehder (nrehder@allgeier.ch)\n",
    "# Create for: SDSC 2024\n",
    "# Date Created: 22.01.2024\n",
    "# Last Updated: 22.01.2024\n",
    "# Python Version: 3.10.4\n",
    "\n",
    "#General Sources:\n",
    "# https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/assistant\n",
    "# https://learn.microsoft.com/en-us/azure/ai-services/openai/assistants-quickstart?tabs=command-line&pivots=programming-language-studio\n",
    "\n",
    "#Azure Openai Usage:\n",
    "# https://stackoverflow.com/questions/77986927/in-azure-openai-assistants-when-i-upload-a-file-and-save-it-where-is-that-file-s\n",
    "# https://techcommunity.microsoft.com/t5/fasttrack-for-azure/strategies-for-optimizing-high-volume-token-usage-with-azure/ba-p/4007751#:~:text=Understanding%20tokens%20and%20limits%20in,generation%2C%20translation%2C%20or%20summarization.\n",
    "\n",
    "#Additionals:\n",
    "# https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models\n",
    "\n",
    "# Download Python packages (run the below command in terminal if packages have not yet been installed)\n",
    "# pip install -r C:\\Python\\sdsc\\requirements.txt\n",
    "\n",
    "#————————————————————"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "from io import StringIO\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Azure OpenAI Key and Endpoint. These values can be found within the Azure OpenAI Service resource in portal.azure.com under Keys and Endpoint\n",
    "load_dotenv(dotenv_path=Path(\"C:\\Python\\openai-lab\\.venv\\.env\"))\n",
    "azure_oai_key = os.environ['AZURE_OPENAI_KEY']\n",
    "azure_oai_endpoint = os.environ['AZURE_OPENAI_ENDPOINT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Azure OpenAI client\n",
    "client = AzureOpenAI(\n",
    "    api_key = azure_oai_key,  \n",
    "    api_version = \"2024-02-15-preview\",\n",
    "    azure_endpoint = azure_oai_endpoint\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileObject(id='assistant-oSLUoxdYPGRzUs9bo42Ta6Ar', bytes=2821745, created_at=1710515972, filename='openfoodfacts.csv', object='file', purpose='assistants', status='processed', status_details=None)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "path_input = r\"C:\\Python\\openai-lab\\data\\openfoodfacts.csv\" #Change path if required\n",
    "\n",
    "# send the csv file to the assistant purpose files\n",
    "response = client.files.create(\n",
    "  file=open(path_input, \"rb\"),\n",
    "  purpose=\"assistants\"\n",
    ")\n",
    "print(response)\n",
    "file__id = response.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = '''\n",
    "You are a senior data analyst who will work with data in an csv file.\n",
    "You have access to a sandboxed environment for writing python code.\n",
    "When the user asks you to perform your actions, you will use the provided csv file.\n",
    "You will perform data cleansing and transformation steps.\n",
    "Execute each of the steps listed below in your ACTIONS section.\n",
    "\n",
    "ACTIONS:\n",
    "\n",
    "1. Read the csv file into a pandas DataFrame by tab separated values.\n",
    "2. Keep only the columns \"product_name\", \"level_1\", \"level_2\", \"level_3\", \"energy-kcal_100g\", \"fat_100g\", \"carbohydrates_100g\", \"sugars_100g\" and \"proteins_100g\" in that order.\n",
    "3. Rename the columns \"energy-kcal_100g\", \"fat_100g\", \"carbohydrates_100g\", \"sugars_100g\" and \"proteins_100g\" to \"energy\", \"fat\", \"carbohydrates\", \"sugars\" and \"proteins\"\n",
    "4. Trim and lowercase the values of all columns.\n",
    "5. Remove rows with non-roman character such as Arabic, Chinese, Cyrillic, Greek, Hebrew, Japanese, Korean, Tamil and Thai from columns \"product_name\", \"level_1\", \"level_2\", \"level_3\".\n",
    "6. Remove rows with missing, empty or NA values from columns \"product_name\", \"level_1\", \"level_2\", \"level_3\".\n",
    "7. Remove duplicate from column \"product_name\".\n",
    "8. Remove smilarily named values column \"product_name\". Smilarity example: \"coconut milk\" and \"coconut milk drink\". Prepare the results as Table_1.\n",
    "8. Prepare Table_1 as an csv file for download by the user. \n",
    "9. Provide a summary paragraph explaining the preparation of the data set.\n",
    "\n",
    "DO NOT:\n",
    "1. Do not return any images. \n",
    "2. Do not return any other file types.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an assistant\n",
    "assistant = client.beta.assistants.create(\n",
    "    name = \"data analyst assistant\",\n",
    "    instructions = instructions,\n",
    "    tools = [{\"type\": \"code_interpreter\"}],\n",
    "    model = \"gpt-4-1106-preview\", #You must replace this value with the deployment name for your model.\n",
    "    file_ids=[file__id]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant(id='asst_PQicQwkHBxkcMeJiqMJUJK8C', created_at=1710516004, description=None, file_ids=['assistant-oSLUoxdYPGRzUs9bo42Ta6Ar'], instructions='\\nYou are a senior data analyst who will work with data in an csv file.\\nYou have access to a sandboxed environment for writing python code.\\nWhen the user asks you to perform your actions, you will use the provided csv file.\\nYou will perform data cleansing and transformation steps.\\nExecute each of the steps listed below in your ACTIONS section.\\n\\nACTIONS:\\n\\n1. Read the csv file into a pandas DataFrame by tab separated values.\\n2. Keep only the columns \"product_name\", \"level_1\", \"level_2\", \"level_3\", \"energy-kcal_100g\", \"fat_100g\", \"carbohydrates_100g\", \"sugars_100g\" and \"proteins_100g\" in that order.\\n3. Rename the columns \"energy-kcal_100g\", \"fat_100g\", \"carbohydrates_100g\", \"sugars_100g\" and \"proteins_100g\" to \"energy\", \"fat\", \"carbohydrates\", \"sugars\" and \"proteins\"\\n4. Trim and lowercase the values of all columns.\\n5. Remove rows with non-roman character such as Arabic, Chinese, Cyrillic, Greek, Hebrew, Japanese, Korean, Tamil and Thai from columns \"product_name\", \"level_1\", \"level_2\", \"level_3\".\\n6. Remove rows with missing, empty or NA values from columns \"product_name\", \"level_1\", \"level_2\", \"level_3\".\\n7. Remove duplicate from column \"product_name\".\\n8. Remove smilarily named values column \"product_name\". Smilarity example: \"coconut milk\" and \"coconut milk drink\". Prepare the results as Table_1.\\n8. Prepare Table_1 as an csv file for download by the user. \\n9. Provide a summary paragraph explaining the preparation of the data set.\\n\\nDO NOT:\\n1. Do not return any images. \\n2. Do not return any other file types.\\n', metadata={}, model='gpt-4-1106-preview', name='data analyst assistant', object='assistant', tools=[ToolCodeInterpreter(type='code_interpreter')])\n"
     ]
    }
   ],
   "source": [
    "# Get the file id\n",
    "fileId = assistant.file_ids[0]\n",
    "print(assistant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "thread = client.beta.threads.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a user prompt to the thread\n",
    "\n",
    "prompt = \"Please execute your ACTIONS on the data stored in the csv file \" + fileId\n",
    "\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id = thread.id,\n",
    "    role = \"user\",\n",
    "    content = prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Assistant\n",
    "\n",
    "run = client.beta.threads.runs.create(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=assistant.id,\n",
    "  #instructions=\"New instructions\" #You can optionally provide new instructions but these will override the default instructions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in progress...\n",
      "in progress...\n",
      "in progress...\n",
      "in progress...\n",
      "in progress...\n",
      "in progress...\n",
      "Assistant: The data set has been prepared and the processed data is available for download as a CSV file. \n",
      "\n",
      "The following actions were performed on the data:\n",
      "- The data was read from the CSV file with tab-separated values into a pandas DataFrame.\n",
      "- Only columns related to product information and nutritional values were retained.\n",
      "- Columns were renamed for clarity.\n",
      "- Values were trimmed and lowercased to maintain consistency.\n",
      "- Rows containing non-roman characters in the product and category columns were removed to ensure readability.\n",
      "- Incomplete rows with missing information in these columns were also discarded.\n",
      "- Duplicate product names were eliminated.\n",
      "- Products with similar names were identified and removed based on substring matching to avoid redundancies.\n",
      "- The final cleaned data was saved to a new CSV file named `cleaned_data.csv`.\n",
      "\n",
      "The sanitized and deduplicated set is now more reliable for further analysis or application within data-driven systems.\n",
      "\n",
      "You can download the cleaned data set using the link below:\n",
      "\n",
      "[Download the cleaned data set (cleaned_data.csv)](sandbox:/mnt/data/cleaned_data.csv)\n",
      "Assistant: The process for removing similarly named products from the \"product_name\" column has taken too long and was automatically interrupted. The method implemented to check for similarly named products is not efficient for larger datasets.\n",
      "\n",
      "I will simplify the approach for checking similar names to improve performance. Let's try a more efficient method and continue where it left off.\n",
      "User: Please execute your ACTIONS on the data stored in the csv file assistant-oSLUoxdYPGRzUs9bo42Ta6Ar\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    sec = 30\n",
    "    # Wait for 30 seconds\n",
    "    time.sleep(sec)  \n",
    "    # Retrieve the run status\n",
    "    run_status = client.beta.threads.runs.retrieve(\n",
    "        thread_id=thread.id,\n",
    "        run_id=run.id\n",
    "    )\n",
    "    # If run is completed, get messages\n",
    "    if run_status.status == 'completed':\n",
    "        messages = client.beta.threads.messages.list(\n",
    "            thread_id=thread.id\n",
    "        )\n",
    "        # Loop through messages and print content based on role\n",
    "        for msg in messages.data:\n",
    "            role = msg.role\n",
    "            try:\n",
    "                content = msg.content[0].text.value\n",
    "                print(f\"{role.capitalize()}: {content}\")\n",
    "            except AttributeError:\n",
    "                # This will execute if .text does not exist\n",
    "                print(f\"{role.capitalize()}: [Non-text content, possibly an image or other file type]\")\n",
    "        break\n",
    "    elif run.status == \"requires_action\":\n",
    "        # handle function calling and continue with the execution\n",
    "        pass\n",
    "    elif run.status == \"expired\" or run.status==\"failed\" or run.status==\"cancelled\":\n",
    "        # run failed, expired, or was cancelled\n",
    "        break    \n",
    "    else:\n",
    "        print(\"in progress...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to read xlsx files from Azure Openai\n",
    "\n",
    "def read_and_save_file(first_file_id, file_name):    \n",
    "    # its binary, so read it and then make it a file like object\n",
    "    file_data = client.files.content(first_file_id)\n",
    "    file_data_bytes = file_data.read()\n",
    "    file_like_object = io.BytesIO(file_data_bytes)\n",
    "    #now read as csv to create df\n",
    "    returned_data = pd.read_csv(file_like_object)\n",
    "    returned_data.to_csv(file_name, index=False)\n",
    "    return returned_data\n",
    "    # file = read_and_save_file(first_file_id, \"analyst_output.csv\")\n",
    "    \n",
    "def files_from_messages(messages, asst_name):\n",
    "    first_thread_message = messages.data[0]  # Accessing the first ThreadMessage\n",
    "    message_ids = first_thread_message.file_ids\n",
    "    print(message_ids)\n",
    "    # Loop through each file ID and save the file with a sequential name\n",
    "    for i, file_id in enumerate(message_ids):\n",
    "        file_name = f\"{asst_name}_output_{i+1}.csv\"  # Generate a sequential file name\n",
    "        read_and_save_file(file_id, file_name)\n",
    "        print(f'saved {file_name}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['assistant-4XjxBtAK4yCqFe0eR6mpb9RD']\n",
      "saved data_analyst_assistant_output_1.csv\n"
     ]
    }
   ],
   "source": [
    "# extract the file names from the response and retrieve the content\n",
    "asst_name = 'data_analyst_assistant'        \n",
    "files_from_messages(messages, asst_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean up\n",
    "\n",
    "client.beta.assistants.delete(assistant.id)\n",
    "client.beta.threads.delete(thread.id)\n",
    "for i in client.files.list():\n",
    "    client.files.delete(i.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in client.beta.assistants.list():\n",
    "    client.beta.assistants.delete(i.id)\n",
    "for i in client.beta.threads.list():\n",
    "    client.beta.threads.delete(i.id)\n",
    "for i in client.files.list():\n",
    "    client.files.delete(i.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "\n",
    "    # Load data\n",
    "    path_input = r\"C:\\Python\\data\\openfoodfacts.xlsx\" #Change path if required\n",
    "\n",
    "    # send the csv file to the assistant purpose files\n",
    "    response = client.files.create(\n",
    "    file=open(path_input, \"rb\"),\n",
    "    purpose=\"assistants\"\n",
    "    )\n",
    "\n",
    "    file__id = response.id\n",
    "\n",
    "    # Create an assistant\n",
    "    assistant = client.beta.assistants.create(\n",
    "        name = \"data analyst assistant \" + str(i),\n",
    "        instructions = instructions,\n",
    "        tools = [{\"type\": \"code_interpreter\"}],\n",
    "        model = \"gpt-4-1106-preview\", #You must replace this value with the deployment name for your model.\n",
    "        file_ids=[file__id]\n",
    "    )\n",
    "\n",
    "    fileId = assistant.file_ids[0]\n",
    "\n",
    "    thread = client.beta.threads.create()\n",
    "\n",
    "    prompt = \"Please execute your ACTIONS on the data stored in the xlsx file \" + fileId\n",
    "\n",
    "    message = client.beta.threads.messages.create(\n",
    "        thread_id = thread.id,\n",
    "        role = \"user\",\n",
    "        content = prompt\n",
    "    )\n",
    "\n",
    "    run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id,\n",
    "    #instructions=\"New instructions\" #You can optionally provide new instructions but these will override the default instructions\n",
    "    )\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
