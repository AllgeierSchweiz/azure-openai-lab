{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#————————————————————\n",
    "\n",
    "# Name: Azure OpenAI Assistant for Nutritional Values (V1)\n",
    "\n",
    "# Purpose:\n",
    "\n",
    "\n",
    "# Company: Allgeier Schweiz AG\n",
    "# Author: Nicolas Rehder (nrehder@allgeier.ch)\n",
    "# Create for: SDSC 2024\n",
    "# Date Created: 22.01.2024\n",
    "# Last Updated: 22.01.2024\n",
    "# Python Version: 3.10.4\n",
    "\n",
    "#General Sources:\n",
    "# https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/assistant\n",
    "# https://learn.microsoft.com/en-us/azure/ai-services/openai/assistants-quickstart?tabs=command-line&pivots=programming-language-studio\n",
    "\n",
    "#Azure Openai Usage:\n",
    "# https://stackoverflow.com/questions/77986927/in-azure-openai-assistants-when-i-upload-a-file-and-save-it-where-is-that-file-s\n",
    "# https://techcommunity.microsoft.com/t5/fasttrack-for-azure/strategies-for-optimizing-high-volume-token-usage-with-azure/ba-p/4007751#:~:text=Understanding%20tokens%20and%20limits%20in,generation%2C%20translation%2C%20or%20summarization.\n",
    "\n",
    "#Additionals:\n",
    "# https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models\n",
    "\n",
    "# Download Python packages (run the below command in terminal if packages have not yet been installed)\n",
    "# pip install -r C:\\Python\\sdsc\\requirements.txt\n",
    "\n",
    "#————————————————————"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "from io import StringIO\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Azure OpenAI Key and Endpoint. These values can be found within the Azure OpenAI Service resource in portal.azure.com under Keys and Endpoint\n",
    "load_dotenv(dotenv_path=Path(\"C:\\Python\\openai-lab\\.venv\\.env\"))\n",
    "azure_oai_key = os.environ['AZURE_OPENAI_KEY']\n",
    "azure_oai_endpoint = os.environ['AZURE_OPENAI_ENDPOINT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Azure OpenAI client\n",
    "client = AzureOpenAI(\n",
    "    api_key = azure_oai_key,  \n",
    "    api_version = \"2024-02-15-preview\",\n",
    "    azure_endpoint = azure_oai_endpoint\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileObject(id='assistant-RfxYH5aPCpuEIr4jaZXA5MQk', bytes=18276416, created_at=1710246487, filename='openfoodfacts.xlsx', object='file', purpose='assistants', status='processed', status_details=None)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "path_input = r\"C:\\Python\\data\\openfoodfacts.xlsx\" #Change path if required\n",
    "\n",
    "# send the csv file to the assistant purpose files\n",
    "response = client.files.create(\n",
    "  file=open(path_input, \"rb\"),\n",
    "  purpose=\"assistants\"\n",
    ")\n",
    "print(response)\n",
    "file__id = response.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = '''\n",
    "You are a senior data analyst who will work with data in an xlsx file.\n",
    "You have access to a sandboxed environment for writing python code.\n",
    "When the user asks you to perform your actions, you will use the provided xlsx file.\n",
    "You will perform data cleansing and transformation steps.\n",
    "Execute each of the steps listed below in your ACTIONS section.\n",
    "\n",
    "ACTIONS:\n",
    "\n",
    "1. Read the xlsx file into a pandas DataFrame.\n",
    "2. Keep only the columns \"product_name\", \"level_1\", \"level_2\", \"level_3\".\n",
    "3. Trim and lowercase the values of columns \"product_name\", \"level_1\", \"level_2\", \"level_3\".\n",
    "4. Remove rows with non-roman character such as Arabic, Chinese, Cyrillic, Greek, Hebrew, Japanese, Korean, Tamil and Thai from columns \"product_name\", \"level_1\", \"level_2\", \"level_3\".\n",
    "5. Remove rows with missing, empty or NA values from columns \"product_name\", \"level_1\", \"level_2\", \"level_3\".\n",
    "6. Remove duplicate values from column \"product_name\" and prepare the results as Table_1.\n",
    "7. Prepare Table_1 as an xlsx file for download by the user. \n",
    "8. Provide a summary paragraph explaining the preparation of the data set.\n",
    "\n",
    "DO NOT:\n",
    "1. Do not return any images. \n",
    "2. Do not return any other file types.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an assistant\n",
    "assistant = client.beta.assistants.create(\n",
    "    name = \"data analyst assistant\",\n",
    "    instructions = instructions,\n",
    "    tools = [{\"type\": \"code_interpreter\"}],\n",
    "    model = \"gpt-4-1106-preview\", #You must replace this value with the deployment name for your model.\n",
    "    file_ids=[file__id]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant(id='asst_JXiCvERyfKilB6aAh4Eu9Pm3', created_at=1710246506, description=None, file_ids=['assistant-RfxYH5aPCpuEIr4jaZXA5MQk'], instructions='\\nYou are a senior data analyst who will work with data in an xlsx file.\\nYou have access to a sandboxed environment for writing python code.\\nWhen the user asks you to perform your actions, you will use the provided xlsx file.\\nYou will perform data cleansing and transformation steps.\\nExecute each of the steps listed below in your ACTIONS section.\\n\\nACTIONS:\\n\\n1. Read the xlsx file into a pandas DataFrame.\\n2. Keep only the columns \"product_name\", \"level_1\", \"level_2\", \"level_3\".\\n3. Trim and lowercase the values of columns \"product_name\", \"level_1\", \"level_2\", \"level_3\".\\n4. Remove non-alphanumeric characters from column \"product_name\".\\n5. Remove empty or NA rows from columns \"product_name\", \"level_1\", \"level_2\", \"level_3\".\\n6. Remove duplicate values from column \"product_name\" and prepare the results as Table_1.\\n7. Prepare Table_1 as an xlsx file for download by the user. \\n8. Provide a summary paragraph explaining the preparation of the data set.\\n\\nDO NOT:\\n1. Do not return any images. \\n2. Do not return any other file types.\\n', metadata={}, model='gpt-4-1106-preview', name='data analyst assistant', object='assistant', tools=[ToolCodeInterpreter(type='code_interpreter')])\n"
     ]
    }
   ],
   "source": [
    "# Get the file id\n",
    "fileId = assistant.file_ids[0]\n",
    "print(assistant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "thread = client.beta.threads.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a user prompt to the thread\n",
    "\n",
    "prompt = \"Please execute your ACTIONS on the data stored in the xlsx file \" + fileId\n",
    "\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id = thread.id,\n",
    "    role = \"user\",\n",
    "    content = prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Assistant\n",
    "\n",
    "run = client.beta.threads.runs.create(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=assistant.id,\n",
    "  #instructions=\"New instructions\" #You can optionally provide new instructions but these will override the default instructions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in progress...\n",
      "in progress...\n",
      "in progress...\n",
      "in progress...\n",
      "Assistant: The data set preparation is complete. Here's a summary of the steps performed:\n",
      "\n",
      "1. I read the provided Excel file into a pandas DataFrame.\n",
      "2. I retained only the relevant columns: \"product_name\", \"level_1\", \"level_2\", and \"level_3\".\n",
      "3. I trimmed whitespace and converted the string values to lowercase in these columns to ensure consistency.\n",
      "4. I removed any non-alphanumeric characters from the \"product_name\" column to clean the product names.\n",
      "5. I discarded rows with missing or NA values in any of the four columns to maintain data integrity.\n",
      "6. I eliminated duplicate entries from the \"product_name\" column to ensure that each product name is unique in the final table, now referred to as Table_1.\n",
      "7. I have prepared Table_1 as an xlsx file ready for your download:\n",
      "\n",
      "[Download the cleaned product data xlsx file](sandbox:/mnt/data/cleaned_product_data.xlsx)\n",
      "\n",
      "If you require any further analysis or modifications, please let me know.\n",
      "User: Please execute your ACTIONS on the data stored in the xlsx file assistant-RfxYH5aPCpuEIr4jaZXA5MQk\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    sec = 30\n",
    "    # Wait for 30 seconds\n",
    "    time.sleep(sec)  \n",
    "    # Retrieve the run status\n",
    "    run_status = client.beta.threads.runs.retrieve(\n",
    "        thread_id=thread.id,\n",
    "        run_id=run.id\n",
    "    )\n",
    "    # If run is completed, get messages\n",
    "    if run_status.status == 'completed':\n",
    "        messages = client.beta.threads.messages.list(\n",
    "            thread_id=thread.id\n",
    "        )\n",
    "        # Loop through messages and print content based on role\n",
    "        for msg in messages.data:\n",
    "            role = msg.role\n",
    "            try:\n",
    "                content = msg.content[0].text.value\n",
    "                print(f\"{role.capitalize()}: {content}\")\n",
    "            except AttributeError:\n",
    "                # This will execute if .text does not exist\n",
    "                print(f\"{role.capitalize()}: [Non-text content, possibly an image or other file type]\")\n",
    "        break\n",
    "    elif run.status == \"requires_action\":\n",
    "        # handle function calling and continue with the execution\n",
    "        pass\n",
    "    elif run.status == \"expired\" or run.status==\"failed\" or run.status==\"cancelled\":\n",
    "        # run failed, expired, or was cancelled\n",
    "        break    \n",
    "    else:\n",
    "        print(\"in progress...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to read xlsx files from Azure Openai\n",
    "\n",
    "def read_and_save_file(first_file_id, file_name):    \n",
    "    # its binary, so read it and then make it a file like object\n",
    "    file_data = client.files.content(first_file_id)\n",
    "    file_data_bytes = file_data.read()\n",
    "    file_like_object = io.BytesIO(file_data_bytes)\n",
    "    #now read as csv to create df\n",
    "    returned_data = pd.read_excel(file_like_object)\n",
    "    returned_data.to_excel(file_name, index=False)\n",
    "    return returned_data\n",
    "    # file = read_and_save_file(first_file_id, \"analyst_output.csv\")\n",
    "    \n",
    "def files_from_messages(messages, asst_name):\n",
    "    first_thread_message = messages.data[0]  # Accessing the first ThreadMessage\n",
    "    message_ids = first_thread_message.file_ids\n",
    "    print(message_ids)\n",
    "    # Loop through each file ID and save the file with a sequential name\n",
    "    for i, file_id in enumerate(message_ids):\n",
    "        file_name = f\"{asst_name}_output_{i+1}.xlsx\"  # Generate a sequential file name\n",
    "        read_and_save_file(file_id, file_name)\n",
    "        print(f'saved {file_name}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the file names from the response and retrieve the content\n",
    "asst_name = 'data_analyst_assistant'        \n",
    "files_from_messages(messages, asst_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean up\n",
    "\n",
    "client.beta.assistants.delete(assistant.id)\n",
    "client.beta.threads.delete(thread.id)\n",
    "for i in client.files.list():\n",
    "    client.files.delete(i.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in client.beta.assistants.list():\n",
    "    client.beta.assistants.delete(i.id)\n",
    "for i in client.beta.threads.list():\n",
    "    client.beta.threads.delete(i.id)\n",
    "for i in client.files.list():\n",
    "    client.files.delete(i.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test run 50 people\n",
    "\n",
    "for i in range(50):\n",
    "\n",
    "    # Load data\n",
    "    path_input = r\"C:\\Python\\data\\openfoodfacts.xlsx\" #Change path if required\n",
    "\n",
    "    # send the csv file to the assistant purpose files\n",
    "    response = client.files.create(\n",
    "    file=open(path_input, \"rb\"),\n",
    "    purpose=\"assistants\"\n",
    "    )\n",
    "\n",
    "    file__id = response.id\n",
    "\n",
    "    # Create an assistant\n",
    "    assistant = client.beta.assistants.create(\n",
    "        name = \"data analyst assistant \" + str(i),\n",
    "        instructions = instructions,\n",
    "        tools = [{\"type\": \"code_interpreter\"}],\n",
    "        model = \"gpt-4-1106-preview\", #You must replace this value with the deployment name for your model.\n",
    "        file_ids=[file__id]\n",
    "    )\n",
    "\n",
    "    fileId = assistant.file_ids[0]\n",
    "\n",
    "    thread = client.beta.threads.create()\n",
    "\n",
    "    prompt = \"Please execute your ACTIONS on the data stored in the xlsx file \" + fileId\n",
    "\n",
    "    message = client.beta.threads.messages.create(\n",
    "        thread_id = thread.id,\n",
    "        role = \"user\",\n",
    "        content = prompt\n",
    "    )\n",
    "\n",
    "    run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id,\n",
    "    #instructions=\"New instructions\" #You can optionally provide new instructions but these will override the default instructions\n",
    "    )\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
