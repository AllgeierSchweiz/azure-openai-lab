{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#————————————————————\n",
    "\n",
    "# Name: Azure OpenAI Assistant API, Data Pre-processing for Fine-Tuning\n",
    "\n",
    "# Purpose:  This notebook will use the Azure OpenAI Assistant API to conduct data pre-processing steps on the recipes CSV for fine-tuning.\n",
    "\n",
    "# Company: Allgeier Schweiz AG\n",
    "# Author: Nicolas Rehder (nrehder@allgeier.ch), Alex Dean (adean@allgeier.ch)\n",
    "# Create for: SDSC 2024\n",
    "# Date Created: 22.01.2024\n",
    "# Last Updated: 25.05.2024\n",
    "# Python Version: 3.10.4\n",
    "\n",
    "# Troubleshooting:\n",
    "# \n",
    "\n",
    "# Download Python packages (run the below command in terminal if packages have not yet been installed)\n",
    "# pip install -r C:\\Python\\sdsc\\requirements.txt\n",
    "\n",
    "#————————————————————"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python packages\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "from io import StringIO\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load required variables from env file.\n",
    "load_dotenv(dotenv_path=Path(\"/workspaces/azure-openai-lab/.venv/.env\")) #Error sometimes due to \\ or \\\\. Try one or the other. \"C:\\\\Python\\\\azure-openai-lab\\\\.venv\\\\.env\"\n",
    "\n",
    "# Load Azure OpenAI Key and Endpoint. These values can be found within the Azure OpenAI Service resource in portal.azure.com under Keys and Endpoint\n",
    "azure_oai_key = os.environ['AZURE_OPENAI_KEY']\n",
    "azure_oai_endpoint = os.environ['AZURE_OPENAI_ENDPOINT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Azure OpenAI client\n",
    "client = AzureOpenAI(\n",
    "    api_key = azure_oai_key,  \n",
    "    api_version = \"2024-02-15-preview\",\n",
    "    azure_endpoint = azure_oai_endpoint\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileObject(id='assistant-yJjDRA4iOizy89uOxKD9jUGR', bytes=107440, created_at=1716316089, filename='recipes-preprocessed.csv', object='file', purpose='assistants', status='processed', status_details=None)\n"
     ]
    }
   ],
   "source": [
    "# Upload file into Azure OpenAI Service [NOT USED IN WORKSHOP]\n",
    "# path_input = r\"C:\\Python\\azure-openai-lab\\data\\recipes-preprocessed.csv\" #Change path if required\n",
    "\n",
    "# # send the csv file to the assistant purpose files\n",
    "# response = client.files.create(\n",
    "#   file=open(path_input, \"rb\"),\n",
    "#   purpose=\"assistants\"\n",
    "# )\n",
    "# print(response)\n",
    "# file__id = response.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant-yJjDRA4iOizy89uOxKD9jUGR\n"
     ]
    }
   ],
   "source": [
    "# Import existing uploaded file on Azure OpenAI Service\n",
    "for i in client.files.list():\n",
    "    if \"recipes-preprocessed\" in i.filename:\n",
    "        file__id = i.id\n",
    "        print(i.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data transformation instructions\n",
    "instructions = '''\n",
    "### INSTRUCTIONS\n",
    "You are a senior data analyst who will work with data in an csv file.\n",
    "You have access to a sandboxed environment for writing python code.\n",
    "The objective is to create a datset for fine-tuning. The dataset must be formatted in the conversational format that is used by the Chat completions API.\n",
    "An example of the conversational format is available in the EXAMPLES section.\n",
    "When the user asks you to perform your actions, you will use the provided csv file and examples in the EXAMPLE section.\n",
    "Execute each of the steps listed below in your ACTIONS section.\n",
    "\n",
    "---\n",
    "\n",
    "### EXAMPLES:\n",
    "\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"This is a recipe generator. The recipe generated should be output as a JSON object.\"}, {\"role\": \"user\", \"content\": \"Create a flavourful recipe from a list of ingredients and provide the output as a JSON object\"}, {\"role\": \"user\", \"content\": \"firm silken tofu, fresh raspberries, agave nectar, lemon juice, water, sea salt\"}, {\"role\": \"assistant\", \"content\": \"{\"name\":\"vegan tofu berry custard\",\"minutes\":10,\"tags\":\"['15-minutes-or-less', 'time-to-make', 'course', 'main-ingredient', 'preparation', 'occasion', 'desserts', 'fruit', 'dinner-party', 'summer', 'vegan', 'vegetarian', 'puddings-and-mousses', 'dietary', 'seasonal', 'berries']\",\"nutrition\":\"[97.2, 4.0, 19.0, 7.0, 13.0, 1.0, 4.0]\",\"n_steps\":5,\"steps\":\"['put berries in a saucepan with the water and sweetener , bring to the boil , and then lower the heat , and simmer until tender and a syrup forms', 'cool', 'put everything in your blender with the berries , and puree until smooth and creamy', 'add in more lemon juice and sweetener to taste if desired', 'chill in the fridge , and serve topped with more fresh berries and vegan cream']\",\"description\":\"it just doesnu2019t get much easier than this delicious summer pudding or custard.rit can be prepared in less than 10 minutes (and that includes getting the ingredients out of your fridge), and is absolutely delicious. i got this recipe from my friend alison about 10 years ago, and my snout has paid homage to her fabulous berry custard trough countless times since. this refreshing chilled treat makes a fantastic, simple, last-minute dessert; great after school snack; or guilt free treat on a hot summeru2019s day. either way it is soy berry good! you could substitute any berries you like for this recipe. if you are allergic to soy u2013 fresh young thai coconut meat and coconut water works really well as a substitute.\",\"ingredients\":\"['firm silken tofu', 'fresh raspberries', 'agave nectar', 'lemon juice', 'water', 'sea salt']\",\"n_ingredients\":6}\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"This is a recipe generator. The recipe generated should be output as a JSON object.\"}, {\"role\": \"user\", \"content\": \"Create a flavourful recipe from a list of ingredients and provide the output as a JSON object\"}, {\"role\": \"user\", \"content\": \"vegetable oil, onion, garlic cloves, eggplants, basil, oregano, salt, pepper, red pepper, yellow pepper, zucchini, tomato paste, chickpeas, tomatoes, fresh basil\"}, {\"role\": \"assistant\", \"content\": \"{\"name\":\"ratatouille with chickpeas crock pot\",\"minutes\":255,\"tags\":\"['main-ingredient', 'cuisine', 'preparation', 'low-protein', 'healthy', 'vegetables', 'french', 'easy', 'european', 'low-fat', 'vegan', 'vegetarian', 'crock-pot-slow-cooker', 'dietary', 'low-sodium', 'low-cholesterol', 'low-saturated-fat', 'low-calorie', 'low-carb', 'healthy-2', 'low-in-something', 'peppers', 'squash', 'equipment', '3-steps-or-less']\",\"nutrition\":\"[219.5, 6.0, 41.0, 24.0, 17.0, 2.0, 13.0]\",\"n_steps\":9,\"steps\":\"['in a large skillet , heat oil over medium heat , cook onion , garlic , eggplant , basil , oregano , salt & pepper , stirring occasionally , until onion is softened , about 10 minutes', 'scrape into crockpot', 'halve , core , and seed peppers', 'cut into 1 inch pieces', 'cut zucchini into half lengthwise , cut crosswise into 1 1 / 2 inch chunks', 'add to crockpot', 'add tomato paste , chickpeas , and tomatoes , breaking up tomatoes with a spoon', 'cover and cook on low for 4 hours , or until vegetables are tender', 'stir in basil / parsley']\",\"description\":\"a fresh tasting take on the classic french dish, done in a crockpot.  from canadian living.\",\"ingredients\":\"['vegetable oil', 'onion', 'garlic cloves', 'eggplants', 'basil', 'oregano', 'salt', 'pepper', 'red pepper', 'yellow pepper', 'zucchini', 'tomato paste', 'chickpeas', 'tomatoes', 'fresh basil']\",\"n_ingredients\":15}\"}]}\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ACTIONS:\n",
    "\n",
    "1. Read the tab separated comma file data.\n",
    "2. Transform the data and create a jsonl file formatted in the conversational format as shown in the EXAMPLES section.\n",
    "3. The conversational format has a system, user and assistant text input stored inside an array of dictionaries.\n",
    "4. The system text input is always \"Act as a head chef and create a flavourful recipe from a list of ingredients\".\n",
    "5. The user text input takes the list of ingredients in the column \"ingredients\" of the CSV file.\n",
    "6. The assistant input takes all the columns of the CSV file.\n",
    "7. Split the data set into training and testing data sets with a 25% split.\n",
    "8. Make sure both data sets have the same format provided by the EXAMPLES section.\n",
    "9. Name the data set with 75% of the data \"recipes-training-set\".\n",
    "10. Name the data set with 25% of the data \"recipes-validation-set\".\n",
    "11. Prepare both data sets as a jsonl file for download by the user.\n",
    "12. Provide a summary paragraph explaining the preparation of the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### DO NOT:\n",
    "1. Do not return any images. \n",
    "2. Do not return any other file types.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Azure OpenAI Assistant\n",
    "assistant = client.beta.assistants.create(\n",
    "    name = \"data analyst assistant\",\n",
    "    instructions = instructions,\n",
    "    tools = [{\"type\": \"code_interpreter\"}],\n",
    "    model = \"gpt-4-1106-preview\", #\"gpt-4-0125-preview\", #You must replace this value with the deployment name for your model.\n",
    "    file_ids=[file__id]\n",
    ")\n",
    "\n",
    "# Get the file id\n",
    "fileId = assistant.file_ids[0]\n",
    "\n",
    "# Create a thread\n",
    "thread = client.beta.threads.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalize thread and start data transformation using the Azure OpenAI Assistant Code Interpreter\n",
    "prompt = \"Please execute the INSTRUCTIONS and ACTIONS on the data stored in the CSV file using the EXAMPLES as reference for the output format \" + fileId\n",
    "\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id = thread.id,\n",
    "    role = \"user\",\n",
    "    content = prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Azure OpenAI Assistant\n",
    "run = client.beta.threads.runs.create(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=assistant.id,\n",
    "  #instructions=\"New instructions\" #You can optionally provide new instructions but these will override the default instructions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in progress...\n",
      "in progress...\n",
      "in progress...\n",
      "in progress...\n",
      "in progress...\n",
      "in progress...\n",
      "in progress...\n",
      "in progress...\n",
      "Assistant: The datasets have been successfully transformed into the conversational format and split into a training set and a validation set. The resulting jsonl files have been saved as \"recipes-training-set.jsonl\" and \"recipes-validation-set.jsonl\".\n",
      "\n",
      "Here is a summary paragraph explaining the preparation of the dataset:\n",
      "\n",
      "To prepare the dataset, the original CSV file was read and loaded into a pandas DataFrame, taking care to correctly handle commas and quotes that indicate fields containing lists or descriptions. Each row of the dataset was subsequently transformed into the conversational format as specified by the instructions, which includes a system prompt, a user input containing the list of ingredients, and an assistant response formatted with all relevant recipe information. After the transformation, the data was split according to a 75% / 25% ratio for training and validation purposes. Lastly, both training and validation datasets were written to .jsonl files in the specified conversational format, ready for downstream tasks such as fine-tuning language models for conversational applications.\n",
      "\n",
      "The jsonl files are now available for download:\n",
      "- [recipes-training-set.jsonl](sandbox:/mnt/data/recipes-training-set.jsonl)\n",
      "- [recipes-validation-set.jsonl](sandbox:/mnt/data/recipes-validation-set.jsonl)\n",
      "Assistant: There was an error in the code because the `json` module wasn't imported. I will correct this mistake by importing the module and then re-running the steps to create and save the training and validation datasets in jsonl format.\n",
      "Assistant: The data has been successfully loaded into a pandas DataFrame. Now, I will proceed with transforming the data into the conversational format shown in the EXAMPLES section, and then split the dataset into training and validation sets following the 75% / 25% split ratio.\n",
      "Assistant: The first line of the file is indeed structured with commas as the delimiter for the columns. The second line, however, reveals that the delimiter between records is a comma, but there are quotes around fields that contain lists or descriptions, which may also contain commas. This is a standard CSV format, and we can use the `pandas` library's CSV reading functionality to handle this correctly by specifying that quotes should be considered.\n",
      "\n",
      "I will proceed with reading the file again, specifying the correct parameters for handling the quotes around fields that contain lists or descriptions.\n",
      "Assistant: It seems that there's an issue with the data loading process, as the entire row is being read as a single column. This could possibly be because the delimiter used might be incorrect, or the data itself may not be well-structured. I'll make another attempt to read the file correctly by trying different delimiters or reviewing the file structure to ensure proper loading of the data.\n",
      "User: Please execute the INSTRUCTIONS and ACTIONS on the data stored in the CSV file using the EXAMPLES as reference for the output format assistant-yJjDRA4iOizy89uOxKD9jUGR\n"
     ]
    }
   ],
   "source": [
    "# Check status of Azure OpenAI Assistant run\n",
    "while True:\n",
    "    sec = 30\n",
    "    # Wait for 30 seconds\n",
    "    time.sleep(sec)  \n",
    "    # Retrieve the run status\n",
    "    run_status = client.beta.threads.runs.retrieve(\n",
    "        thread_id=thread.id,\n",
    "        run_id=run.id\n",
    "    )\n",
    "    # If run is completed, get messages\n",
    "    if run_status.status == 'completed':\n",
    "        messages = client.beta.threads.messages.list(\n",
    "            thread_id=thread.id\n",
    "        )\n",
    "        # Loop through messages and print content based on role\n",
    "        for msg in messages.data:\n",
    "            role = msg.role\n",
    "            try:\n",
    "                content = msg.content[0].text.value\n",
    "                print(f\"{role.capitalize()}: {content}\")\n",
    "            except AttributeError:\n",
    "                # This will execute if .text does not exist\n",
    "                print(f\"{role.capitalize()}: [Non-text content, possibly an image or other file type]\")\n",
    "        break\n",
    "    elif run.status == \"requires_action\":\n",
    "        # handle function calling and continue with the execution\n",
    "        pass\n",
    "    elif run.status == \"expired\" or run.status==\"failed\" or run.status==\"cancelled\":\n",
    "        # run failed, expired, or was cancelled\n",
    "        break   \n",
    "    # elif run.last_error != \"None\":\n",
    "    #     # run failed, expired, or was cancelled\n",
    "    #     break  \n",
    "    else:\n",
    "        print(\"in progress...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['assistant-rCY7clhjRgszoJkfcSdytXm2', 'assistant-vSOHoUpYrtcgYOsenfCs14O5']\n"
     ]
    }
   ],
   "source": [
    "# Functions to read xlsx files from Azure Openai\n",
    "\n",
    "output_path = r\"/workspaces/azure-openai-lab/data/generated_output/\" #r\"C:\\\\Python\\\\azure-openai-lab\\\\data\\\\generated_output\\\\\"\n",
    "\n",
    "# Write to jsonl\n",
    "def write_jsonl(data_list: list, filename: str) -> None:\n",
    "    with open(filename, \"w\") as out:\n",
    "        for ddict in data_list:\n",
    "            jout = json.dumps(ddict) + \"\\n\"\n",
    "            out.write(jout)\n",
    "\n",
    "\n",
    "def read_and_save_file(first_file_id, file_name, output_path):   \n",
    "    # its binary, so read it and then make it a file like object\n",
    "    file_data = client.files.content(first_file_id)\n",
    "    file_data_bytes = file_data.read()\n",
    "    file_data_decoded = file_data_bytes.decode('utf8').replace(\"'\", '\"')\n",
    "    file_data_list = file_data_decoded.splitlines()\n",
    "    write_jsonl(file_data_list, output_path + file_name)\n",
    "\n",
    "    \n",
    "def files_from_messages():\n",
    "    messages = client.beta.threads.messages.list(\n",
    "            thread_id=thread.id\n",
    "        )\n",
    "    first_thread_message = messages.data[0]  # Accessing the first ThreadMessage\n",
    "    message_ids = first_thread_message.file_ids\n",
    "    print(message_ids)\n",
    "    # Loop through each file ID and save the file with a sequential name\n",
    "    for i, file_id in enumerate(message_ids):\n",
    "        if i == 1:\n",
    "            file_name = f\"recipes-training-set.jsonl\"  # Generate a sequential file name\n",
    "            read_and_save_file(file_id, file_name, output_path)\n",
    "        else:\n",
    "            file_name = f\"recipes-validation-set.jsonl\"  # Generate a sequential file name\n",
    "            read_and_save_file(file_id, file_name, output_path)\n",
    "\n",
    "# Extract the file names from the response, retrieve the content and save the data as a jsonl file\n",
    "files_from_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean up Azure OpenAI environment\n",
    "client.beta.assistants.delete(assistant.id)\n",
    "client.beta.threads.delete(thread.id)\n",
    "for i in range(0, 2):\n",
    "    client.files.delete(messages.data[0].file_ids[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
