{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#————————————————————\n",
    "\n",
    "# Name: Azure OpenAI Assistant API, Data Pre-processing for Fine-Tuning\n",
    "\n",
    "# Purpose:  This notebook will use the Azure OpenAI Assistant API to conduct data pre-processing steps on the recipes CSV for fine-tuning.\n",
    "\n",
    "# Company: Allgeier Schweiz AG\n",
    "# Author: Nicolas Rehder (nrehder@allgeier.ch), Alex Dean (adean@allgeier.ch)\n",
    "# Create for: SDSC 2024\n",
    "# Date Created: 22.01.2024\n",
    "# Last Updated: 25.05.2024\n",
    "# Python Version: 3.10.4\n",
    "\n",
    "# Troubleshooting:\n",
    "# \n",
    "\n",
    "# Download Python packages (run the below command in terminal if packages have not yet been installed)\n",
    "# pip install -r C:\\Python\\sdsc\\requirements.txt\n",
    "\n",
    "#————————————————————"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python packages\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "from io import StringIO\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load required variables from env file.\n",
    "load_dotenv(dotenv_path=Path(\"/workspaces/azure-openai-lab/.venv/.env\")) #Error sometimes due to \\ or \\\\. Try one or the other. \"C:\\\\Python\\\\azure-openai-lab\\\\.venv\\\\.env\"\n",
    "\n",
    "# Load Azure OpenAI Key and Endpoint. These values can be found within the Azure OpenAI Service resource in portal.azure.com under Keys and Endpoint\n",
    "azure_oai_key = os.environ['AZURE_OPENAI_KEY']\n",
    "azure_oai_endpoint = os.environ['AZURE_OPENAI_ENDPOINT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Azure OpenAI client\n",
    "client = AzureOpenAI(\n",
    "    api_key = azure_oai_key,  \n",
    "    api_version = \"2024-02-15-preview\",\n",
    "    azure_endpoint = azure_oai_endpoint\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileObject(id='assistant-yJjDRA4iOizy89uOxKD9jUGR', bytes=107440, created_at=1716316089, filename='recipes-preprocessed.csv', object='file', purpose='assistants', status='processed', status_details=None)\n"
     ]
    }
   ],
   "source": [
    "# Upload file into Azure OpenAI Service [NOT USED IN WORKSHOP]\n",
    "# path_input = r\"C:\\Python\\azure-openai-lab\\data\\recipes-preprocessed.csv\" #Change path if required\n",
    "\n",
    "# # send the csv file to the assistant purpose files\n",
    "# response = client.files.create(\n",
    "#   file=open(path_input, \"rb\"),\n",
    "#   purpose=\"assistants\"\n",
    "# )\n",
    "# print(response)\n",
    "# file__id = response.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant-yJjDRA4iOizy89uOxKD9jUGR\n"
     ]
    }
   ],
   "source": [
    "# Import existing uploaded file on Azure OpenAI Service\n",
    "for i in client.files.list():\n",
    "    if \"recipes-preprocessed\" in i.filename:\n",
    "        file__id = i.id\n",
    "        print(i.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data transformation instructions\n",
    "instructions = '''\n",
    "### INSTRUCTIONS\n",
    "You are a senior data analyst who will work with data in an csv file.\n",
    "You have access to a sandboxed environment for writing python code.\n",
    "The objective is to create a datset for fine-tuning. The dataset must be formatted in the conversational format that is used by the Chat completions API.\n",
    "An example of the conversational format is available in the EXAMPLES section.\n",
    "When the user asks you to perform your actions, you will use the provided csv file and examples in the EXAMPLE section.\n",
    "Execute each of the steps listed below in your ACTIONS section.\n",
    "\n",
    "---\n",
    "\n",
    "### EXAMPLES:\n",
    "\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Act as a head chef and create a flavourful recipe from a list of ingredients\"}, {\"role\": \"user\", \"content\": \"'pork spareribs', 'soy sauce', 'fresh garlic', 'fresh ginger', 'chili powder', 'fresh coarse ground black pepper', 'salt', 'fresh cilantro leaves', 'tomato sauce', 'brown sugar', 'yellow onion', 'white vinegar', 'honey', 'a.1. original sauce', 'liquid smoke', 'cracked black pepper', 'cumin', 'dry mustard', 'cinnamon sticks', 'orange, juice of', 'mirin', 'water'\"}, {\"role\": \"assistant\", \"content\": \"{\"name\":\"backyard style  barbecued ribs\",\"minutes\":120,\"tags\":\"['weeknight', 'time-to-make', 'course', 'main-ingredient', 'cuisine', 'preparation', 'occasion', 'north-american', 'south-west-pacific', 'main-dish', 'pork', 'oven', 'holiday-event', 'stove-top', 'hawaiian', 'spicy', 'copycat', 'independence-day', 'meat', 'pork-ribs', 'super-bowl', 'novelty', 'taste-mood', 'savory', 'sweet', 'equipment', '4-hours-or-less']\",\"nutrition\":\"[1109.5, 83.0, 378.0, 275.0, 96.0, 86.0, 36.0]\",\"n_steps\":10,\"steps\":\"['in a medium saucepan combine all the ingredients for sauce#1 , bring to a full rolling boil , reduce heat to medium low and simmer for 1 hour , stirring often', 'rub the ribs with soy sauce , garlic , ginger , chili powder , pepper , salt and chopped cilantro , both sides !', 'wrap ribs in heavy duty foil', 'let stand 1 hour', 'preheat oven to 350 degrees', 'place ribs in oven for 1 hour , turning once after 30 minutes', '3 times during cooking the ribs open foil wrap and drizzle ribs with sauce#1', 'place all the ingredients for sauce#2 in a glass or plastic bowl , whisk well and set aside', 'remove ribs from oven and place on serving platter', 'offer both sauces at table to drizzle over ribs']\",\"description\":\"this recipe is posted by request and was originaly from chef sam choy's cookbook \",\"ingredients\":\"['pork spareribs', 'soy sauce', 'fresh garlic', 'fresh ginger', 'chili powder', 'fresh coarse ground black pepper', 'salt', 'fresh cilantro leaves', 'tomato sauce', 'brown sugar', 'yellow onion', 'white vinegar', 'honey', 'a.1. original sauce', 'liquid smoke', 'cracked black pepper', 'cumin', 'dry mustard', 'cinnamon sticks', 'orange, juice of', 'mirin', 'water']\",\"n_ingredients\":22}\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Act as a head chef and create a flavourful recipe from a list of ingredients\"}, {\"role\": \"user\", \"content\": \"'lean pork chops', 'flour', 'salt', 'dry mustard', 'garlic powder', 'oil', 'chicken rice soup'\"}, {\"role\": \"assistant\", \"content\": \"{\"name\":\"chicken lickin  good  pork chops\",\"minutes\":500,\"tags\":\"['weeknight', 'time-to-make', 'course', 'main-ingredient', 'preparation', 'main-dish', 'pork', 'crock-pot-slow-cooker', 'dietary', 'meat', 'pork-chops', 'equipment']\",\"nutrition\":\"[105.7, 8.0, 0.0, 26.0, 5.0, 4.0, 3.0]\",\"n_steps\":5,\"steps\":\"['dredge pork chops in mixture of flour , salt , dry mustard and garlic powder', 'brown in oil in a large skillet', 'place browned pork chops in a crock pot', 'add the can of soup , undiluted', 'cover and cook on low for 6-8 hours']\",\"description\":\"here's and old standby i enjoy from time to time. it's from an old newspaper clipping i cut out years ago. very tasty.\",\"ingredients\":\"['lean pork chops', 'flour', 'salt', 'dry mustard', 'garlic powder', 'oil', 'chicken rice soup']\",\"n_ingredients\":7}\"}]}\n",
    "\n",
    "---\n",
    "\n",
    "### ACTIONS:\n",
    "\n",
    "1. Read the tab separated comma file data.\n",
    "2. Transform the data and create a jsonl file formatted in the conversational format as shown in the EXAMPLES section.\n",
    "3. The conversational format has a system, user and assistant text input stored inside an array of dictionaries.\n",
    "4. The system text input is always \"Act as a head chef and create a flavourful recipe from a list of ingredients\".\n",
    "5. The user text input takes the list of ingredients in the column \"ingredients\" of the CSV file.\n",
    "6. The assistant input takes all the columns of the CSV file.\n",
    "7. Split the data set into training and testing data sets with a 25% split.\n",
    "8. Make sure both data sets have the same format provided by the EXAMPLES section.\n",
    "9. Name the data set with 75% of the data \"recipes-training-set\".\n",
    "10. Name the data set with 25% of the data \"recipes-validation-set\".\n",
    "11. Prepare both data sets as a jsonl file for download by the user.\n",
    "12. Provide a summary paragraph explaining the preparation of the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### DO NOT:\n",
    "1. Do not return any images. \n",
    "2. Do not return any other file types.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Azure OpenAI Assistant\n",
    "assistant = client.beta.assistants.create(\n",
    "    name = \"data analyst assistant\",\n",
    "    instructions = instructions,\n",
    "    tools = [{\"type\": \"code_interpreter\"}],\n",
    "    model = \"gpt-4-1106-preview\", #\"gpt-4-0125-preview\", #You must replace this value with the deployment name for your model.\n",
    "    file_ids=[file__id]\n",
    ")\n",
    "\n",
    "# Get the file id\n",
    "fileId = assistant.file_ids[0]\n",
    "\n",
    "# Create a thread\n",
    "thread = client.beta.threads.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalize thread and start data transformation using the Azure OpenAI Assistant Code Interpreter\n",
    "prompt = \"Please execute the INSTRUCTIONS and ACTIONS on the data stored in the CSV file using the EXAMPLES as reference for the output format \" + fileId\n",
    "\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id = thread.id,\n",
    "    role = \"user\",\n",
    "    content = prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Azure OpenAI Assistant\n",
    "run = client.beta.threads.runs.create(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=assistant.id,\n",
    "  #instructions=\"New instructions\" #You can optionally provide new instructions but these will override the default instructions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in progress...\n",
      "in progress...\n",
      "in progress...\n",
      "in progress...\n",
      "in progress...\n",
      "in progress...\n",
      "in progress...\n",
      "in progress...\n",
      "Assistant: The datasets have been successfully transformed into the conversational format and split into a training set and a validation set. The resulting jsonl files have been saved as \"recipes-training-set.jsonl\" and \"recipes-validation-set.jsonl\".\n",
      "\n",
      "Here is a summary paragraph explaining the preparation of the dataset:\n",
      "\n",
      "To prepare the dataset, the original CSV file was read and loaded into a pandas DataFrame, taking care to correctly handle commas and quotes that indicate fields containing lists or descriptions. Each row of the dataset was subsequently transformed into the conversational format as specified by the instructions, which includes a system prompt, a user input containing the list of ingredients, and an assistant response formatted with all relevant recipe information. After the transformation, the data was split according to a 75% / 25% ratio for training and validation purposes. Lastly, both training and validation datasets were written to .jsonl files in the specified conversational format, ready for downstream tasks such as fine-tuning language models for conversational applications.\n",
      "\n",
      "The jsonl files are now available for download:\n",
      "- [recipes-training-set.jsonl](sandbox:/mnt/data/recipes-training-set.jsonl)\n",
      "- [recipes-validation-set.jsonl](sandbox:/mnt/data/recipes-validation-set.jsonl)\n",
      "Assistant: There was an error in the code because the `json` module wasn't imported. I will correct this mistake by importing the module and then re-running the steps to create and save the training and validation datasets in jsonl format.\n",
      "Assistant: The data has been successfully loaded into a pandas DataFrame. Now, I will proceed with transforming the data into the conversational format shown in the EXAMPLES section, and then split the dataset into training and validation sets following the 75% / 25% split ratio.\n",
      "Assistant: The first line of the file is indeed structured with commas as the delimiter for the columns. The second line, however, reveals that the delimiter between records is a comma, but there are quotes around fields that contain lists or descriptions, which may also contain commas. This is a standard CSV format, and we can use the `pandas` library's CSV reading functionality to handle this correctly by specifying that quotes should be considered.\n",
      "\n",
      "I will proceed with reading the file again, specifying the correct parameters for handling the quotes around fields that contain lists or descriptions.\n",
      "Assistant: It seems that there's an issue with the data loading process, as the entire row is being read as a single column. This could possibly be because the delimiter used might be incorrect, or the data itself may not be well-structured. I'll make another attempt to read the file correctly by trying different delimiters or reviewing the file structure to ensure proper loading of the data.\n",
      "User: Please execute the INSTRUCTIONS and ACTIONS on the data stored in the CSV file using the EXAMPLES as reference for the output format assistant-yJjDRA4iOizy89uOxKD9jUGR\n"
     ]
    }
   ],
   "source": [
    "# Check status of Azure OpenAI Assistant run\n",
    "while True:\n",
    "    sec = 30\n",
    "    # Wait for 30 seconds\n",
    "    time.sleep(sec)  \n",
    "    # Retrieve the run status\n",
    "    run_status = client.beta.threads.runs.retrieve(\n",
    "        thread_id=thread.id,\n",
    "        run_id=run.id\n",
    "    )\n",
    "    # If run is completed, get messages\n",
    "    if run_status.status == 'completed':\n",
    "        messages = client.beta.threads.messages.list(\n",
    "            thread_id=thread.id\n",
    "        )\n",
    "        # Loop through messages and print content based on role\n",
    "        for msg in messages.data:\n",
    "            role = msg.role\n",
    "            try:\n",
    "                content = msg.content[0].text.value\n",
    "                print(f\"{role.capitalize()}: {content}\")\n",
    "            except AttributeError:\n",
    "                # This will execute if .text does not exist\n",
    "                print(f\"{role.capitalize()}: [Non-text content, possibly an image or other file type]\")\n",
    "        break\n",
    "    elif run.status == \"requires_action\":\n",
    "        # handle function calling and continue with the execution\n",
    "        pass\n",
    "    elif run.status == \"expired\" or run.status==\"failed\" or run.status==\"cancelled\":\n",
    "        # run failed, expired, or was cancelled\n",
    "        break   \n",
    "    # elif run.last_error != \"None\":\n",
    "    #     # run failed, expired, or was cancelled\n",
    "    #     break  \n",
    "    else:\n",
    "        print(\"in progress...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['assistant-rCY7clhjRgszoJkfcSdytXm2', 'assistant-vSOHoUpYrtcgYOsenfCs14O5']\n"
     ]
    }
   ],
   "source": [
    "# Functions to read xlsx files from Azure Openai\n",
    "\n",
    "output_path = r\"/workspaces/azure-openai-lab/data/generated_output/\" #r\"C:\\\\Python\\\\azure-openai-lab\\\\data\\\\generated_output\\\\\"\n",
    "\n",
    "# Write to jsonl\n",
    "def write_jsonl(data_list: list, filename: str) -> None:\n",
    "    with open(filename, \"w\") as out:\n",
    "        for ddict in data_list:\n",
    "            jout = json.dumps(ddict) + \"\\n\"\n",
    "            out.write(jout)\n",
    "\n",
    "\n",
    "def read_and_save_file(first_file_id, file_name, output_path):   \n",
    "    # its binary, so read it and then make it a file like object\n",
    "    file_data = client.files.content(first_file_id)\n",
    "    file_data_bytes = file_data.read()\n",
    "    file_data_decoded = file_data_bytes.decode('utf8').replace(\"'\", '\"')\n",
    "    file_data_list = file_data_decoded.splitlines()\n",
    "    write_jsonl(file_data_list, output_path + file_name)\n",
    "\n",
    "    \n",
    "def files_from_messages():\n",
    "    messages = client.beta.threads.messages.list(\n",
    "            thread_id=thread.id\n",
    "        )\n",
    "    first_thread_message = messages.data[0]  # Accessing the first ThreadMessage\n",
    "    message_ids = first_thread_message.file_ids\n",
    "    print(message_ids)\n",
    "    # Loop through each file ID and save the file with a sequential name\n",
    "    for i, file_id in enumerate(message_ids):\n",
    "        if i == 1:\n",
    "            file_name = f\"recipes-training-set.jsonl\"  # Generate a sequential file name\n",
    "            read_and_save_file(file_id, file_name, output_path)\n",
    "        else:\n",
    "            file_name = f\"recipes-validation-set.jsonl\"  # Generate a sequential file name\n",
    "            read_and_save_file(file_id, file_name, output_path)\n",
    "\n",
    "# Extract the file names from the response, retrieve the content and save the data as a jsonl file\n",
    "files_from_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean up Azure OpenAI environment\n",
    "client.beta.assistants.delete(assistant.id)\n",
    "client.beta.threads.delete(thread.id)\n",
    "for i in range(0, 2):\n",
    "    client.files.delete(messages.data[0].file_ids[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
