{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#————————————————————\n",
    "\n",
    "# Name: Azure OpenAI Assistant API, Data Pre-processing for Fine-Tuning\n",
    "\n",
    "# Purpose:  This notebook will use the Azure OpenAI Assistant API to conduct data pre-processing steps on the recipes CSV using for RAG.\n",
    "\n",
    "# Company: Allgeier Schweiz AG\n",
    "# Author: Nicolas Rehder (nrehder@allgeier.ch), Alex Dean (adean@allgeier.ch)\n",
    "# Create for: SDSC 2024\n",
    "# Date Created: 22.01.2024\n",
    "# Last Updated: 19.05.2024\n",
    "# Python Version: 3.10.4\n",
    "\n",
    "#General Sources:\n",
    "# https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/assistant\n",
    "# https://learn.microsoft.com/en-us/azure/ai-services/openai/assistants-quickstart?tabs=command-line&pivots=programming-language-studio\n",
    "\n",
    "#Azure Openai Usage:\n",
    "# https://stackoverflow.com/questions/77986927/in-azure-openai-assistants-when-i-upload-a-file-and-save-it-where-is-that-file-s\n",
    "# https://techcommunity.microsoft.com/t5/fasttrack-for-azure/strategies-for-optimizing-high-volume-token-usage-with-azure/ba-p/4007751#:~:text=Understanding%20tokens%20and%20limits%20in,generation%2C%20translation%2C%20or%20summarization.\n",
    "\n",
    "#Additionals:\n",
    "# https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models\n",
    "\n",
    "# Download Python packages (run the below command in terminal if packages have not yet been installed)\n",
    "# pip install -r C:\\Python\\sdsc\\requirements.txt\n",
    "\n",
    "#————————————————————"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python packages\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "from io import StringIO\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load required variables from env file.\n",
    "load_dotenv(dotenv_path=Path(\"/workspaces/azure-openai-lab/.venv/.env\")) #Error sometimes due to \\ or \\\\. Try one or the other. \"C:\\\\Python\\\\azure-openai-lab\\\\.venv\\\\.env\"\n",
    "\n",
    "# Load Azure OpenAI Key and Endpoint. These values can be found within the Azure OpenAI Service resource in portal.azure.com under Keys and Endpoint\n",
    "azure_oai_key = os.environ['AZURE_OPENAI_KEY']\n",
    "azure_oai_endpoint = os.environ['AZURE_OPENAI_ENDPOINT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Azure OpenAI client\n",
    "client = AzureOpenAI(\n",
    "    api_key = azure_oai_key,  \n",
    "    api_version = \"2024-02-15-preview\",\n",
    "    azure_endpoint = azure_oai_endpoint\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileObject(id='assistant-yJjDRA4iOizy89uOxKD9jUGR', bytes=107440, created_at=1716316089, filename='recipes-preprocessed.csv', object='file', purpose='assistants', status='processed', status_details=None)\n"
     ]
    }
   ],
   "source": [
    "# Upload file into Azure OpenAI Service [NOT USED IN WORKSHOP]\n",
    "# path_input = r\"C:\\Python\\azure-openai-lab\\data\\recipes-preprocessed.csv\" #Change path if required\n",
    "\n",
    "# # send the csv file to the assistant purpose files\n",
    "# response = client.files.create(\n",
    "#   file=open(path_input, \"rb\"),\n",
    "#   purpose=\"assistants\"\n",
    "# )\n",
    "# print(response)\n",
    "# file__id = response.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant-rkRgo4cBcZwAG05AnplGlfM7\n"
     ]
    }
   ],
   "source": [
    "# Import existing uploaded file on Azure OpenAI Service\n",
    "for i in client.files.list():\n",
    "    if \"recipes-preprocessed\" in i.filename:\n",
    "        file__id = i.id\n",
    "        print(i.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data transformation instructions\n",
    "instructions = '''\n",
    "You are a senior data analyst who will work with data in an csv file.\n",
    "You have access to a sandboxed environment for writing python code.\n",
    "The objective is to create a datset for fine-tuning. The dataset must be formatted in the conversational format that is used by the Chat completions API.\n",
    "An example of the conversational format is available in the EXAMPLES section.\n",
    "When the user asks you to perform your actions, you will use the provided csv file and examples in the EXAMPLE section.\n",
    "Execute each of the steps listed below in your ACTIONS section.\n",
    "\n",
    "EXAMPLES:\n",
    "\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Act as a head chef and create a flavourful recipe from a list of ingredients\"}, {\"role\": \"user\", \"content\": \"'pork spareribs', 'soy sauce', 'fresh garlic', 'fresh ginger', 'chili powder', 'fresh coarse ground black pepper', 'salt', 'fresh cilantro leaves', 'tomato sauce', 'brown sugar', 'yellow onion', 'white vinegar', 'honey', 'a.1. original sauce', 'liquid smoke', 'cracked black pepper', 'cumin', 'dry mustard', 'cinnamon sticks', 'orange, juice of', 'mirin', 'water'\"}, {\"role\": \"assistant\", \"content\": \"{\"name\":\"backyard style  barbecued ribs\",\"minutes\":120,\"tags\":\"['weeknight', 'time-to-make', 'course', 'main-ingredient', 'cuisine', 'preparation', 'occasion', 'north-american', 'south-west-pacific', 'main-dish', 'pork', 'oven', 'holiday-event', 'stove-top', 'hawaiian', 'spicy', 'copycat', 'independence-day', 'meat', 'pork-ribs', 'super-bowl', 'novelty', 'taste-mood', 'savory', 'sweet', 'equipment', '4-hours-or-less']\",\"nutrition\":\"[1109.5, 83.0, 378.0, 275.0, 96.0, 86.0, 36.0]\",\"n_steps\":10,\"steps\":\"['in a medium saucepan combine all the ingredients for sauce#1 , bring to a full rolling boil , reduce heat to medium low and simmer for 1 hour , stirring often', 'rub the ribs with soy sauce , garlic , ginger , chili powder , pepper , salt and chopped cilantro , both sides !', 'wrap ribs in heavy duty foil', 'let stand 1 hour', 'preheat oven to 350 degrees', 'place ribs in oven for 1 hour , turning once after 30 minutes', '3 times during cooking the ribs open foil wrap and drizzle ribs with sauce#1', 'place all the ingredients for sauce#2 in a glass or plastic bowl , whisk well and set aside', 'remove ribs from oven and place on serving platter', 'offer both sauces at table to drizzle over ribs']\",\"description\":\"this recipe is posted by request and was originaly from chef sam choy's cookbook \",\"ingredients\":\"['pork spareribs', 'soy sauce', 'fresh garlic', 'fresh ginger', 'chili powder', 'fresh coarse ground black pepper', 'salt', 'fresh cilantro leaves', 'tomato sauce', 'brown sugar', 'yellow onion', 'white vinegar', 'honey', 'a.1. original sauce', 'liquid smoke', 'cracked black pepper', 'cumin', 'dry mustard', 'cinnamon sticks', 'orange, juice of', 'mirin', 'water']\",\"n_ingredients\":22}\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Act as a head chef and create a flavourful recipe from a list of ingredients\"}, {\"role\": \"user\", \"content\": \"'lean pork chops', 'flour', 'salt', 'dry mustard', 'garlic powder', 'oil', 'chicken rice soup'\"}, {\"role\": \"assistant\", \"content\": \"{\"name\":\"chicken lickin  good  pork chops\",\"minutes\":500,\"tags\":\"['weeknight', 'time-to-make', 'course', 'main-ingredient', 'preparation', 'main-dish', 'pork', 'crock-pot-slow-cooker', 'dietary', 'meat', 'pork-chops', 'equipment']\",\"nutrition\":\"[105.7, 8.0, 0.0, 26.0, 5.0, 4.0, 3.0]\",\"n_steps\":5,\"steps\":\"['dredge pork chops in mixture of flour , salt , dry mustard and garlic powder', 'brown in oil in a large skillet', 'place browned pork chops in a crock pot', 'add the can of soup , undiluted', 'cover and cook on low for 6-8 hours']\",\"description\":\"here's and old standby i enjoy from time to time. it's from an old newspaper clipping i cut out years ago. very tasty.\",\"ingredients\":\"['lean pork chops', 'flour', 'salt', 'dry mustard', 'garlic powder', 'oil', 'chicken rice soup']\",\"n_ingredients\":7}\"}]}\n",
    "\n",
    "\n",
    "ACTIONS:\n",
    "\n",
    "1. Read the CSV file.\n",
    "2. Transform the data and create a jsonl file formatted in the conversational format as shown in the EXAMPLES section.\n",
    "3. The conversational format has a system, user and assistant text input stored inside an array of dictionaries.\n",
    "4. The system text input is always \"Act as a head chef and create a flavourful recipe from a list of ingredients\".\n",
    "5. The user text input takes the list of ingredients in the column \"ingredients\" of the CSV file.\n",
    "6. The assistant input takes all the columns of the CSV file.\n",
    "7. Split the data set into training and testing data sets with a 25% split.\n",
    "8. Make sure both data sets have the same format provided by the EXAMPLES section.\n",
    "9. Name the data set with 75% of the data \"recipes-training-set\".\n",
    "10. Name the data set with 25% of the data \"recipes-validation-set\".\n",
    "11. Prepare both data sets as a jsonl file for download by the user.\n",
    "12. Provide a summary paragraph explaining the preparation of the dataset.\n",
    "\n",
    "DO NOT:\n",
    "1. Do not return any images. \n",
    "2. Do not return any other file types.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Azure OpenAI Assistant\n",
    "assistant = client.beta.assistants.create(\n",
    "    name = \"data analyst assistant\",\n",
    "    instructions = instructions,\n",
    "    tools = [{\"type\": \"code_interpreter\"}],\n",
    "    model = \"gpt-4-1106-preview\", #\"gpt-4-0125-preview\", #You must replace this value with the deployment name for your model.\n",
    "    file_ids=[file__id]\n",
    ")\n",
    "\n",
    "# Get the file id\n",
    "fileId = assistant.file_ids[0]\n",
    "\n",
    "# Create a thread\n",
    "thread = client.beta.threads.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalize thread and start data transformation using the Azure OpenAI Assistant Code Interpreter\n",
    "prompt = \"Please execute your ACTIONS on the data stored in the CSV file using the EXAMPLES as reference for the dataset format \" + fileId\n",
    "\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id = thread.id,\n",
    "    role = \"user\",\n",
    "    content = prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Azure OpenAI Assistant\n",
    "run = client.beta.threads.runs.create(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=assistant.id,\n",
    "  #instructions=\"New instructions\" #You can optionally provide new instructions but these will override the default instructions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check status of Azure OpenAI Assistant run\n",
    "while True:\n",
    "    sec = 30\n",
    "    # Wait for 30 seconds\n",
    "    time.sleep(sec)  \n",
    "    # Retrieve the run status\n",
    "    run_status = client.beta.threads.runs.retrieve(\n",
    "        thread_id=thread.id,\n",
    "        run_id=run.id\n",
    "    )\n",
    "    # If run is completed, get messages\n",
    "    if run_status.status == 'completed':\n",
    "        messages = client.beta.threads.messages.list(\n",
    "            thread_id=thread.id\n",
    "        )\n",
    "        # Loop through messages and print content based on role\n",
    "        for msg in messages.data:\n",
    "            role = msg.role\n",
    "            try:\n",
    "                content = msg.content[0].text.value\n",
    "                print(f\"{role.capitalize()}: {content}\")\n",
    "            except AttributeError:\n",
    "                # This will execute if .text does not exist\n",
    "                print(f\"{role.capitalize()}: [Non-text content, possibly an image or other file type]\")\n",
    "        break\n",
    "    elif run.status == \"requires_action\":\n",
    "        # handle function calling and continue with the execution\n",
    "        pass\n",
    "    elif run.status == \"expired\" or run.status==\"failed\" or run.status==\"cancelled\":\n",
    "        # run failed, expired, or was cancelled\n",
    "        break   \n",
    "    elif run.last_error != \"None\":\n",
    "        # run failed, expired, or was cancelled\n",
    "        break  \n",
    "    else:\n",
    "        print(\"in progress...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to read xlsx files from Azure Openai\n",
    "\n",
    "output_path = r\"/workspaces/azure-openai-lab/data/\" #r\"C:\\\\Python\\\\azure-openai-lab\\\\data\\\\\"\n",
    "\n",
    "# Write to jsonl\n",
    "def write_jsonl(data_list: list, filename: str) -> None:\n",
    "    with open(filename, \"w\") as out:\n",
    "        for ddict in data_list:\n",
    "            jout = json.dumps(ddict) + \"\\n\"\n",
    "            out.write(jout)\n",
    "\n",
    "\n",
    "def read_and_save_file(first_file_id, file_name, output_path):   \n",
    "    # its binary, so read it and then make it a file like object\n",
    "    file_data = client.files.content(first_file_id)\n",
    "    file_data_bytes = file_data.read()\n",
    "    file_data_decoded = file_data_bytes.decode('utf8').replace(\"'\", '\"')\n",
    "    file_data_list = file_data_decoded.splitlines()\n",
    "    write_jsonl(file_data_list, output_path + file_name)\n",
    "\n",
    "    \n",
    "def files_from_messages():\n",
    "    messages = client.beta.threads.messages.list(\n",
    "            thread_id=thread.id\n",
    "        )\n",
    "    first_thread_message = messages.data[0]  # Accessing the first ThreadMessage\n",
    "    message_ids = first_thread_message.file_ids\n",
    "    print(message_ids)\n",
    "    # Loop through each file ID and save the file with a sequential name\n",
    "    for i, file_id in enumerate(message_ids):\n",
    "        if i == 1:\n",
    "            file_name = f\"recipes-training-set.jsonl\"  # Generate a sequential file name\n",
    "            read_and_save_file(file_id, file_name, output_path)\n",
    "        else:\n",
    "            file_name = f\"recipes-validation-set.jsonl\"  # Generate a sequential file name\n",
    "            read_and_save_file(file_id, file_name, output_path)\n",
    "\n",
    "# Extract the file names from the response, retrieve the content and save the data as a jsonl file\n",
    "files_from_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean up Azure OpenAI environment\n",
    "client.beta.assistants.delete(assistant.id)\n",
    "client.beta.threads.delete(thread.id)\n",
    "client.files.delete(messages.data[0].file_ids[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
