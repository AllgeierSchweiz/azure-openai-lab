{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "#————————————————————\n",
    "\n",
    "# Name: OpenAI Fine-Tuning(V1)\n",
    "# Purpose: Create a fine-tuned openai GPT model that assist in the classification of product categories.\n",
    "# Company: Allgeier Schweiz AG\n",
    "# Author: Nicolas Rehder (nrehder@allgeier.ch)\n",
    "# Create for: SDSC 2024\n",
    "# Date Created: 10.01.2024\n",
    "# Last Updated: 10.01.2024\n",
    "# Python Version: 3.10.4\n",
    "\n",
    "#General Sources:\n",
    "#https://platform.openai.com/docs/api-reference?lang=python\n",
    "#https://medium.com/ai-advances/complete-process-for-fine-tuning-gpt-3-5-turbo-using-openai-api-db4a50b3de1a\n",
    "\n",
    "#Openai Usage:\n",
    "#https://platform.openai.com/usage\n",
    "\n",
    "#Additionals:\n",
    "# model\n",
    "# - ID of model to use.\n",
    "# - https://platform.openai.com/docs/models\n",
    "# - https://openai.com/pricing\n",
    "\n",
    "# messages\n",
    "# - A list of messages comprising the conversation so far.\n",
    "\n",
    "# temperature\n",
    "# - What sampling temperature to use, between 0 and 2.\n",
    "# - Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and \n",
    "#   deterministic.\n",
    "\n",
    "# max_tokens\n",
    "# - The maximum number of tokens to generate in the chat completion.\n",
    "\n",
    "# Download Python packages (run the below command in terminal if packages have not yet been installed)\n",
    "#pip install -r C:\\Python\\openai-lab\\support\\requirements\\requirements.txt\n",
    "\n",
    "#————————————————————"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from pathlib import Path\n",
    "import openai\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OpenAIKey\n",
    "\n",
    "load_dotenv(dotenv_path=Path(\"C:\\Python\\openai-lab\\.venv\\.env\"))\n",
    "API_KEY = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# Initalise Large Language Model (LLM)\n",
    "\n",
    "llm = OpenAI(api_key = API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Model with single input\n",
    "\n",
    "system_prompt = \"You are a helpful data assistant.\"\n",
    "user_prompt = \"How will AI simplify an employees workload?\"\n",
    "\n",
    "\n",
    "response = llm.chat.completions.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': system_prompt},\n",
    "        {'role': 'user', 'content': user_prompt},\n",
    "    ],\n",
    "    temperature=0.5,\n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Model with Continous Conversations\n",
    "# Careful: Since this code is keeping a history of previous questions and responses, the number of tokens passed to the model grows exponentially. \n",
    "# This leads to higher costs and to slowers response rates.\n",
    "\n",
    "system_prompt = \"You are a helpful data assistant.\"\n",
    "\n",
    "messages = [{'role': 'system', 'content': system_prompt}]\n",
    "\n",
    "while True:\n",
    "    if len(messages) <= 4:\n",
    "        user_text = input()\n",
    "\n",
    "        # Add the user message to the conversation history\n",
    "        messages.append({'role': 'user', 'content': user_text})\n",
    "\n",
    "        response = llm.chat.completions.create(\n",
    "            model='gpt-3.5-turbo',\n",
    "            messages=messages,\n",
    "            temperature=0.5,\n",
    "            max_tokens=1024\n",
    "        )\n",
    "\n",
    "        response = response.choices[0].message.content\n",
    "        print(f'Data Assistant: {response}')\n",
    "\n",
    "        # Add grandmas message to the conversation history\n",
    "        messages.append({'role': 'assistant', 'content': response})\n",
    "        \n",
    "    else:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
